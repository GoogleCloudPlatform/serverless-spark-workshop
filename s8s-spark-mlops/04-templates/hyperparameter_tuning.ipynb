{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamater tuning\n",
    "This script does hyperparameter tuning of a Random Forest Classification model for the customer churn prediction experiment-</br>\n",
    "1. Reads source data from BigQuery as a source, \n",
    "2. Writes model to GCS\n",
    "3. Parses and persists model metrics to GCS and BigQuery\n",
    "4. Writes model test results to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Copyright 2022 Google LLC\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import pandas as pd\n",
    "import sys, logging, argparse, random, tempfile, json\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType\n",
    "from pyspark.sql.functions import lit\n",
    "from pathlib import Path as path\n",
    "from google.cloud import storage\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a. Arguments\n",
    "pipelineID = random.randint(1, 10000)\n",
    "projectNbr = \"YOUR_PROJECT_NBR\"\n",
    "projectID = \"YOUR_PROJECT_ID\"\n",
    "userid = \"USER_ID\"\n",
    "displayPrintStatements = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b. Variables \n",
    "appBaseName = \"customer-churn-model\"\n",
    "appNameSuffix = \"hyperparameter-tuning\"\n",
    "appName = f\"{appBaseName}-{appNameSuffix}\"\n",
    "modelBaseNm = appBaseName\n",
    "modelVersion = pipelineID\n",
    "bqDatasetNm = f\"{projectID}.{userid}_customer_churn_ds\"\n",
    "operation = appNameSuffix\n",
    "bigQuerySourceTableFQN = f\"{bqDatasetNm}.training_data\"\n",
    "bigQueryModelTestResultsTableFQN = f\"{bqDatasetNm}.test_predictions\"\n",
    "bigQueryModelMetricsTableFQN = f\"{bqDatasetNm}.model_metrics\"\n",
    "modelBucketUri = f\"gs://{userid}-s8s_model_bucket-{projectNbr}/{modelBaseNm}/{operation}/{modelVersion}\"\n",
    "metricsBucketUri = f\"gs://{userid}-s8s_metrics_bucket-{projectNbr}/{modelBaseNm}/{operation}/{modelVersion}\"\n",
    "scratchBucketUri = f\"s8s-spark-bucket-{projectNbr}/{appBaseName}/pipelineId-{pipelineID}/{appNameSuffix}/\"\n",
    "pipelineExecutionDt = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And constants\n",
    "SPLIT_SEED = 6\n",
    "SPLIT_SPECS = [0.8, 0.2]\n",
    "MAX_DEPTH = [5, 10, 15]\n",
    "MAX_BINS = [24, 32, 40]\n",
    "N_TREES = [25, 30, 35]\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1c. Display input and output\n",
    "if displayPrintStatements:\n",
    "    print(\"Starting hyperparameter tuning for *Customer Churn* experiment\")\n",
    "    print(\".....................................................\")\n",
    "    print(f\"The datetime now is - {pipelineExecutionDt}\")\n",
    "    print(\" \")\n",
    "    print(\"INPUT PARAMETERS\")\n",
    "    print(f\"....pipelineID={pipelineID}\")\n",
    "    print(f\"....projectID={projectID}\")\n",
    "    print(f\"....projectNbr={projectNbr}\")\n",
    "    print(f\"....displayPrintStatements={displayPrintStatements}\")\n",
    "    print(\" \")\n",
    "    print(\"EXPECTED SETUP\")  \n",
    "    print(f\"....BQ Dataset={bqDatasetNm}\")\n",
    "    print(f\"....Model Training Source Data in BigQuery={bigQuerySourceTableFQN}\")\n",
    "    print(f\"....Scratch Bucket for BQ connector=gs://s8s-spark-bucket-{projectNbr}\") \n",
    "    print(f\"....Model Bucket=gs://{userid}-s8s-model-bucket-{projectNbr}\")  \n",
    "    print(f\"....Metrics Bucket=gs://{userid}-s8s-metrics-bucket-{projectNbr}\") \n",
    "    print(\" \")\n",
    "    print(\"OUTPUT\")\n",
    "    print(f\"....Model in GCS={modelBucketUri}\")\n",
    "    print(f\"....Model metrics in GCS={metricsBucketUri}\")  \n",
    "    print(f\"....Model metrics in BigQuery={bigQueryModelMetricsTableFQN}\")      \n",
    "    print(f\"....Model test results in BigQuery={bigQueryModelTestResultsTableFQN}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Spark config\n",
    "print('....Setting Spark config')\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "# Spark configuration setting for writes to BigQuery\n",
    "spark.conf.set(\"parentProject\", projectID)\n",
    "spark.conf.set(\"temporaryGcsBucket\", scratchBucketUri)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Add Python modules\n",
    "sc.addPyFile(f\"gs://{userid}-s8s_code_bucket-{projectNbr}/pyspark/common_utils.py\")\n",
    "import common_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING DATA - READ, SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Read training data\n",
    "print('....Reading training dataset')\n",
    "inputDF = spark.read \\\n",
    "    .format('bigquery') \\\n",
    "    .load(bigQuerySourceTableFQN)\n",
    "\n",
    "\n",
    "# Typecast some columns to the right datatype\n",
    "inputDF = inputDF.withColumn(\"partner\", inputDF.partner.cast('string')) \\\n",
    "    .withColumn(\"dependents\", inputDF.dependents.cast('string')) \\\n",
    "    .withColumn(\"phone_service\", inputDF.phone_service.cast('string')) \\\n",
    "    .withColumn(\"paperless_billing\", inputDF.paperless_billing.cast('string')) \\\n",
    "    .withColumn(\"churn\", inputDF.churn.cast('string')) \\\n",
    "    .withColumn(\"monthly_charges\", inputDF.monthly_charges.cast('float')) \\\n",
    "    .withColumn(\"total_charges\", inputDF.total_charges.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split to training and test datasets\n",
    "print('....Splitting the dataset')\n",
    "trainDF, testDF = inputDF.randomSplit(SPLIT_SPECS, seed=SPLIT_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING & FEATURE ENGINEERING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Pre-process training data\n",
    "print('....Data pre-procesing')\n",
    "dataPreprocessingStagesList = []\n",
    "# 5a. Create and append to pipeline stages - string indexing and one hot encoding\n",
    "for eachCategoricalColumn in common_utils.CATEGORICAL_COLUMN_LIST:\n",
    "    # Category indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=eachCategoricalColumn, outputCol=eachCategoricalColumn + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[eachCategoricalColumn + \"classVec\"])\n",
    "    # Add stages.  This is a lazy operation\n",
    "    dataPreprocessingStagesList += [stringIndexer, encoder]\n",
    "\n",
    "# 5b. Convert label into label indices using the StringIndexer and append to pipeline stages\n",
    "labelStringIndexer = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
    "dataPreprocessingStagesList += [labelStringIndexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature engineering\n",
    "print('....Feature engineering')\n",
    "featureEngineeringStageList = []\n",
    "assemblerInputs = common_utils.NUMERIC_COLUMN_LIST + [c + \"classVec\" for c in common_utils.CATEGORICAL_COLUMN_LIST]\n",
    "featuresVectorAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "featureEngineeringStageList += [featuresVectorAssembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Model training\n",
    "print('....Model training')\n",
    "modelTrainingStageList = []\n",
    "rfClassifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "modelTrainingStageList += [rfClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Create a model training pipeline for stages defined\n",
    "print('....Instantiating pipeline model')\n",
    "pipeline = Pipeline(stages=dataPreprocessingStagesList + featureEngineeringStageList + modelTrainingStageList)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Hyperparameter tuning & cross validation\n",
    "print('....Hyperparameter tuning & cross validation')\n",
    "parameterGrid = (ParamGridBuilder()\n",
    "               .addGrid(modelTrainingStageList[0].maxDepth, MAX_DEPTH)\n",
    "               .addGrid(modelTrainingStageList[0].maxBins, MAX_BINS)\n",
    "               .addGrid(modelTrainingStageList[0].numTrees, N_TREES)\n",
    "               .build())\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "crossValidatorPipeline = CrossValidator(estimator=pipeline,\n",
    "                                 estimatorParamMaps=parameterGrid,\n",
    "                                 evaluator=evaluator,\n",
    "                                 numFolds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Fit the model; Takes tens of minutes, repartition as needed\n",
    "pipelineModel = crossValidatorPipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Persist model to GCS\n",
    "pipelineModel.write().overwrite().save(modelBucketUri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Test the model with the test dataset\n",
    "print('....Testing the model')\n",
    "predictionsDF = pipelineModel.transform(testDF)\n",
    "predictionsDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Persist model testing results to BigQuery\n",
    "predictionsWithPipelineIdDF = predictionsDF.withColumn(\"pipeline_id\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"model_version\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \\\n",
    "                                   .withColumn(\"operation\", lit(appNameSuffix)) \n",
    "\n",
    "predictionsWithPipelineIdDF.write.format('bigquery') \\\n",
    ".mode(\"append\")\\\n",
    ".option('table', bigQueryModelTestResultsTableFQN) \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14a. Parse metrics\n",
    "def fnParseModelMetrics(predictionsDF, labelColumn, operation, boolSubsetOnly):\n",
    "    \"\"\"\n",
    "    Get model metrics\n",
    "    Args:\n",
    "        predictions: predictions\n",
    "        labelColumn: target column\n",
    "        operation: train or test\n",
    "        boolSubsetOnly: boolean for partial(without true, score, prediction) or full metrics \n",
    "    Returns:\n",
    "        metrics: metrics\n",
    "   \"\"\"\n",
    "    \n",
    "    metricLabels = ['area_roc', 'area_prc', 'accuracy', 'f1', 'precision', 'recall']\n",
    "    metricColumns = ['true', 'score', 'prediction']\n",
    "    metricKeys = [f'{operation}_{ml}' for ml in metricLabels] + metricColumns\n",
    "\n",
    "    # Instantiate evaluators\n",
    "    bcEvaluator = BinaryClassificationEvaluator(labelCol=labelColumn)\n",
    "    mcEvaluator = MulticlassClassificationEvaluator(labelCol=labelColumn)\n",
    "\n",
    "    # Capture metrics -> areas, acc, f1, prec, rec\n",
    "    area_roc = round(bcEvaluator.evaluate(predictionsDF, {bcEvaluator.metricName: 'areaUnderROC'}), 5)\n",
    "    area_prc = round(bcEvaluator.evaluate(predictionsDF, {bcEvaluator.metricName: 'areaUnderPR'}), 5)\n",
    "    acc = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"accuracy\"}), 5)\n",
    "    f1 = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"f1\"}), 5)\n",
    "    prec = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"weightedPrecision\"}), 5)\n",
    "    rec = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"weightedRecall\"}), 5)\n",
    "\n",
    "    # Get the true, score, prediction off of the test results dataframe\n",
    "    rocDictionary = common_utils.fnGetTrueScoreAndPrediction(predictionsDF, labelColumn)\n",
    "    true = rocDictionary['true']\n",
    "    score = rocDictionary['score']\n",
    "    prediction = rocDictionary['prediction']\n",
    "\n",
    "    # Create a metric values array\n",
    "    metricValuesArray = []\n",
    "    if boolSubsetOnly:\n",
    "        metricValuesArray.extend((area_roc, area_prc, acc, f1, prec, rec))\n",
    "    else:\n",
    "        metricValuesArray.extend((area_roc, area_prc, acc, f1, prec, rec, true, score, prediction))\n",
    "    \n",
    "    # Zip the keys and values into a dictionary  \n",
    "    metricsDictionary = dict(zip(metricKeys, metricValuesArray))\n",
    "\n",
    "    return metricsDictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14b. Parse & display a subset of the metrics (sans true, score and prediction)\n",
    "hyperParameterTunedModelMetricsSubset = fnParseModelMetrics(predictionsDF, \"label\", \"test\", True)\n",
    "for m, v in hyperParameterTunedModelMetricsSubset.items():\n",
    "    print(f'{m}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14c. Persist metrics to BigQuery\n",
    "metricsDF = spark.createDataFrame(hyperParameterTunedModelMetricsSubset.items(), [\"metric_nm\", \"metric_value\"]) \n",
    "metricsWithPipelineIdDF = metricsDF.withColumn(\"pipeline_id\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"model_version\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \\\n",
    "                                   .withColumn(\"operation\", lit(appNameSuffix)) \n",
    "\n",
    "metricsWithPipelineIdDF.show()\n",
    "metricsWithPipelineIdDF.write.format('bigquery') \\\n",
    ".mode(\"append\")\\\n",
    ".option('table', bigQueryModelMetricsTableFQN) \\\n",
    ".save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14d. Persist metrics subset to GCS\n",
    "blobName = f\"{modelBaseNm}/{appNameSuffix}/{modelVersion}/subset/metrics.json\"\n",
    "common_utils.fnPersistMetrics(urlparse(metricsBucketUri).netloc, hyperParameterTunedModelMetricsSubset, blobName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14e. Persist metrics to GCS\n",
    "hyperParameterTunedModelMetrics = fnParseModelMetrics(predictionsDF, \"label\", \"test\", False)\n",
    "for m, v in hyperParameterTunedModelMetrics.items():\n",
    "    print(f'{m}: {v}')\n",
    "\n",
    "\n",
    "blobName = f\"{modelBaseNm}/{appNameSuffix}/{modelVersion}/full/metrics.json\"\n",
    "common_utils.fnPersistMetrics(urlparse(metricsBucketUri).netloc, hyperParameterTunedModelMetrics, blobName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "serverless_spark": "{\"name\":\"projects/s8s-spark-ml-mlops/locations/us-central1/sessions/vai-2\",\"uuid\":\"8c53d840-5a00-4c72-bf58-413be0fb4d1d\",\"createTime\":\"2022-08-04T03:41:17.312074Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{},\"state\":\"ACTIVE\",\"stateTime\":\"2022-08-04T03:42:28.412751Z\",\"creator\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"runtimeConfig\":{\"containerImage\":\"gcr.io/s8s-spark-ml-mlops/dataproc_serverless_custom_runtime:1.0.3\",\"properties\":{\"spark:spark.executor.cores\":\"8\",\"spark:spark.executor.instances\":\"5\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.eventLog.dir\":\"gs://{userid}-s8s-sphs-974925525028/8c53d840-5a00-4c72-bf58-413be0fb4d1d/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"subnetworkUri\":\"https://www.googleapis.com/compute/v1/projects/s8s-spark-ml-mlops/regions/us-central1/subnetworks/spark-snet\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/s8s-spark-ml-mlops/regions/us-central1/clusters/s8s-sphs-974925525028\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-08-04T03:41:17.312074Z\"}]}",
  "serverless_spark_kernel_name": "remote-7c7e15bf0de294d98b312d6c-pyspark"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
