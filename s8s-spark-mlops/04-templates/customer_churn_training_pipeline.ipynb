{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a212fa6e-2198-41c6-9e69-8d7abb407684",
   "metadata": {},
   "source": [
    "# MLOps with Spark MLLib & Vertex AI Pipelines\n",
    "In the prior lab modules - <br>(1) We authored Spark code in Serverless Spark interactive notebooks in Vertex AI workbench. <br>(2) We then created PySpark scripts off of them and tested them manually on cloud shell. <br> In this notebook -<br> (3) We will create a Vertex AI pipeline for MLOps - essentially chaining together the serverless Spark applications we developed in (2)<br> In a subsequent modules -<br>(4) We will create a Google Cloud Function to execute the pipeline and <br> (5) Finally, we will call that Google Cloud Function via Cloud Scheduler to complete the automation.\n",
    "We will compile the pipeline JSON and use the same to schedule with Cloud Scheduler separately.\n",
    "\n",
    "Dependency: Custom container image for Serverless Spark (already) created as part of (your) Terraform-based environment provisioning if you have created your environment using the Terraform provided in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d667b70c-401a-4811-80e1-a602d03d8348",
   "metadata": {},
   "source": [
    "  Copyright 2022 Google LLC\n",
    " \n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    " \n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    " \n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5a414-f457-4e23-8c8e-62200dc6cfe7",
   "metadata": {},
   "source": [
    "### 1. One time setup of dependencies\n",
    "Uncomment the cell below, and run just the cell, one time ONLY to install necessary libraries - AND THEN comment it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c98b0-89c9-4eca-bd1d-a39ac780c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip3 install --user --upgrade google-cloud-aiplatform==1.11.0 kfp==1.8.11 google-cloud-pipeline-components==1.0.1 --quiet --no-warn-conflicts\n",
    "\n",
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46e6a9-0c11-42fe-9595-3007cfcfb0ef",
   "metadata": {},
   "source": [
    "### 2. Variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ff675-62e7-410f-b619-f1f8d8ab0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path as path\n",
    "from typing import NamedTuple\n",
    "import os\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google_cloud_pipeline_components import aiplatform as vertex_ai_components\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Condition, Input,\n",
    "                        Metrics, Output, component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a129398-d241-47b8-8063-73b605a936d8",
   "metadata": {},
   "source": [
    "#### a. Project specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed589bb-8ed3-4cb0-96c4-462a310d1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "PROJECT_NBR = \"\"\n",
    "UNIQUE_ID = random.randint(1, 10000)\n",
    "WITHOUT_TASK_CACHING = True\n",
    "BYO_NETWORK = True\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    project_id_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = project_id_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "    \n",
    "    \n",
    "    project_nbr_output = !gcloud projects describe $PROJECT_ID --format='value(projectNumber)'\n",
    "    PROJECT_NBR = project_nbr_output[0]\n",
    "    print(\"Project Number: \", PROJECT_NBR)\n",
    "    \n",
    "umsa_output = !gcloud config list account --format \"value(core.account)\"\n",
    "UMSA_FQN = umsa_output[0]\n",
    "print(\"UMSA FQN: \", UMSA_FQN)\n",
    "print(\"UNIQUE ID: \", UNIQUE_ID)\n",
    "\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f040cba-24ce-4550-93a7-b851d732f301",
   "metadata": {},
   "source": [
    "#### b. Local resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231439c-6ae0-4847-8c3f-8ee23c4c5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_BASE_NM = \"customer-churn-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1340b-3af1-4f95-a763-b8a1e7ca1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_SCRATCH_DIR = path(f\"/home/jupyter/scratch/{APP_BASE_NM}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f8ece-197a-4c9e-be28-03619594bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p $LOCAL_SCRATCH_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50a5dc-d66c-4ff7-a408-db038568ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al $LOCAL_SCRATCH_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a57b5-4cfc-48cb-9f88-cd437353b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd $LOCAL_SCRATCH_DIR && pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ff06b-8ac6-499d-a988-50b93d0e58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al /home/jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c7b7a0-4281-4d6b-9cec-7fed2b300b44",
   "metadata": {},
   "source": [
    "#### d. The pre-created resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c9514-0d95-47f7-9671-1c2b5ff73255",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_BUCKET = f\"gs://USER_ID-s8s_code_bucket-{PROJECT_NBR}\"\n",
    "DATA_BUCKET = f\"gs://USER_ID-s8s_data_bucket-{PROJECT_NBR}\"\n",
    "MODEL_BUCKET = f\"gs://USER_ID-s8s_model_bucket-{PROJECT_NBR}\"\n",
    "SCRATCH_BUCKET = f\"s8s-spark-bucket-{PROJECT_NBR}\"\n",
    "BQ_DS_NM = f\"{PROJECT_ID}.USER_ID_customer_churn_ds\"\n",
    "LOCATION = \"YOUR_GCP_LOCATION\"\n",
    "VPC_NM = f\"s8s-vpc-{PROJECT_NBR}\"\n",
    "SUBNET_RESOURCE_URI = f\"projects/{PROJECT_ID}/regions/{LOCATION}/subnetworks/spark-snet\"\n",
    "PERSISTENT_SPARK_HISTORY_SERVER_RESOURCE_URI = f\"projects/{PROJECT_ID}/regions/{LOCATION}/clusters/s8s-sphs-{PROJECT_NBR}\"\n",
    "GCR_REPO_NM = f\"s8s-spark-{PROJECT_NBR}\"\n",
    "DOCKER_IMAGE_TAG = \"YOUR_SPARK_CONTAINER_IMAGE_TAG\"\n",
    "DOCKER_IMAGE_NM = \"customer_churn_image\"\n",
    "DOCKER_IMAGE_FQN = f\"gcr.io/{PROJECT_ID}/{DOCKER_IMAGE_NM}:{DOCKER_IMAGE_TAG}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab3891-5f9d-40f3-b709-80754abe8d1d",
   "metadata": {},
   "source": [
    "#### e. Pipeline entity specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6858b5a-40bd-4705-9a77-2dc3fd97eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID = UNIQUE_ID\n",
    "PIPELINE_NM = f\"USER_ID-{APP_BASE_NM}-pipeline\"\n",
    "PIPELINE_PACKAGE_SRC_LOCAL_PATH = f\"{LOCAL_SCRATCH_DIR}/pipeline_{PIPELINE_ID}.json\"\n",
    "PIPELINE_ROOT_GCS_URI = f\"{MODEL_BUCKET}/{APP_BASE_NM}/pipelines\"\n",
    "\n",
    "print('PIPELINE_ID =',PIPELINE_ID)\n",
    "print('PIPELINE_NM =',PIPELINE_NM)\n",
    "print('PIPELINE_PACKAGE_SRC_LOCAL_PATH =',PIPELINE_PACKAGE_SRC_LOCAL_PATH)\n",
    "print('PIPELINE_ROOT_GCS_URI =',PIPELINE_ROOT_GCS_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a93fba-9714-46dc-b8f7-929331c159dc",
   "metadata": {},
   "source": [
    "#### d. Pipeline stage agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437c615-18eb-4aa1-92ef-65a6af02a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PY_SCRIPTS_FQP = f\"{CODE_BUCKET}/pyspark\"\n",
    "PYSPARK_COMMON_UTILS_SCRIPT_FQP = [f\"{PY_SCRIPTS_FQP}/common_utils.py\"]\n",
    "\n",
    "print('PY_SCRIPTS_FQP =',PY_SCRIPTS_FQP)\n",
    "print('PYSPARK_COMMON_UTILS_SCRIPT_FQP =',PYSPARK_COMMON_UTILS_SCRIPT_FQP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf673b2e-89c6-4eaf-bb8b-69b206f84814",
   "metadata": {},
   "source": [
    "#### e. Data preprocessing stage specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881fbc3-ee62-4969-928d-55a44dd2219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREPROCESSING_BATCH_PREFIX = \"preprocessing\"\n",
    "DATA_PREPROCESSING_BATCH_INSTANCE_ID = f\"USER_ID-{APP_BASE_NM}-{DATA_PREPROCESSING_BATCH_PREFIX}-{UNIQUE_ID}\"\n",
    "DATA_PREPROCESSING_MAIN_PY_SCRIPT = f\"{PY_SCRIPTS_FQP}/preprocessing.py\"\n",
    "\n",
    "DATA_PROCESSING_SINK = f\"{BQ_DS_NM}.training_data\"\n",
    "DATA_PROCESSING_BQ_SINK_URI = f\"bq://{DATA_PROCESSING_SINK}\"\n",
    "\n",
    "DATA_PREPROCESSING_ARGS = [f\"--pipelineID={UNIQUE_ID}\", \\\n",
    "        f\"--projectID={PROJECT_ID}\", \\\n",
    "        f\"--projectNbr={PROJECT_NBR}\", \n",
    "        f\"--displayPrintStatements={True}\"]\n",
    "\n",
    "print('DATA_PREPROCESSING_BATCH_INSTANCE_ID =',DATA_PREPROCESSING_BATCH_INSTANCE_ID)\n",
    "print('DATA_PREPROCESSING_MAIN_PY_SCRIPT =',DATA_PREPROCESSING_MAIN_PY_SCRIPT)\n",
    "print('DATA_PROCESSING_SINK =',DATA_PROCESSING_SINK)\n",
    "print('DATA_PROCESSING_BQ_SINK_URI =',DATA_PROCESSING_BQ_SINK_URI)\n",
    "print('DATA_PREPROCESSING_ARGS =',DATA_PREPROCESSING_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca656c-5ae8-45ab-95a3-873fede76c01",
   "metadata": {},
   "source": [
    "#### f. Dataset registration specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eae873-c8e8-4f0b-9bac-e83c58f64ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MANAGED_DATASET_NM = f\"USER_ID-{APP_BASE_NM}-{UNIQUE_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d69e4b-d088-4293-a88e-5b4d943615c8",
   "metadata": {},
   "source": [
    "#### g. Model specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd7e0e-3b57-4c4d-a148-4357bf816b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TRAINING_BATCH_PREFIX = \"training\"\n",
    "MODEL_TRAINING_BATCH_INSTANCE_ID = f\"USER_ID-{APP_BASE_NM}-{MODEL_TRAINING_BATCH_PREFIX}-{UNIQUE_ID}\"\n",
    "MODEL_TRAINING_MAIN_PY_SCRIPT = f\"{PY_SCRIPTS_FQP}/model_training.py\"\n",
    "MODEL_TRAINING_ARGS = [f\"--pipelineID={UNIQUE_ID}\", \\\n",
    "        f\"--projectID={PROJECT_ID}\", \\\n",
    "        f\"--projectNbr={PROJECT_NBR}\", \n",
    "        f\"--displayPrintStatements={True}\"]\n",
    "\n",
    "MODEL_METRICS_BUCKET_FQP = f\"gs://USER_ID-s8s_metrics_bucket-{PROJECT_NBR}/{APP_BASE_NM}/{MODEL_TRAINING_BATCH_PREFIX}/{UNIQUE_ID}/full/metrics.json\"\n",
    "\n",
    "print('MODEL_TRAINING_BATCH_INSTANCE_ID =',MODEL_TRAINING_BATCH_INSTANCE_ID)\n",
    "print('MODEL_TRAINING_MAIN_PY_SCRIPT =',MODEL_TRAINING_MAIN_PY_SCRIPT)\n",
    "print('MODEL_TRAINING_ARGS =',MODEL_TRAINING_ARGS)\n",
    "print('MODEL_METRICS_BUCKET_FQP =',MODEL_METRICS_BUCKET_FQP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8f31c-44e3-44cc-8eb1-9eadf50c0137",
   "metadata": {},
   "source": [
    "#### h. Hyperparameter tuning specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfbc78c-8d1d-4590-a86d-925b35c3fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition\n",
    "AUPR_THRESHOLD = 0.5\n",
    "AUPR_HYPERTUNE_CONDITION = \"[AUPR_HYPERTUNE]\"\n",
    "\n",
    "HYPERPARAMETER_TUNING_BATCH_PREFIX = \"hyperparameter-tuning\"\n",
    "HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID = f\"USER_ID-{APP_BASE_NM}-{HYPERPARAMETER_TUNING_BATCH_PREFIX}-{UNIQUE_ID}\"\n",
    "HYPERPARAMETER_TUNING_ARGS = [f\"--pipelineID={UNIQUE_ID}\", \\\n",
    "        f\"--projectID={PROJECT_ID}\", \\\n",
    "        f\"--projectNbr={PROJECT_NBR}\", \n",
    "        f\"--displayPrintStatements={True}\"]\n",
    "\n",
    "HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT = f\"{PY_SCRIPTS_FQP}/hyperparameter_tuning.py\"\n",
    "HYPERPARAMETER_TUNING_BUCKET_FQP = f\"gs://USER_ID-s8s_metrics_bucket-{PROJECT_NBR}/{APP_BASE_NM}/{HYPERPARAMETER_TUNING_BATCH_PREFIX}/{UNIQUE_ID}/full/metrics.json\"\n",
    "\n",
    "\n",
    "print('HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID =',HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID)\n",
    "print('HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT =',HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT)\n",
    "print('HYPERPARAMETER_TUNING_ARGS =',HYPERPARAMETER_TUNING_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87e5090-1f28-4022-b3b1-82099710238b",
   "metadata": {},
   "source": [
    "### 3. Initialize Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c54e20-c08a-40cf-8716-09de594df623",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=SCRATCH_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa4fd3-4b64-4946-a4ac-5937ad96a671",
   "metadata": {},
   "source": [
    "### 4. Define custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80172f-b441-4104-98a0-3bf42dff4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.8\",\n",
    "    packages_to_install=[\"numpy==1.21.2\", \"pandas==1.3.3\", \"scikit-learn==0.24.2\"],\n",
    ")\n",
    "def fnEvaluateModel(\n",
    "    metricsUri: str,\n",
    "    metrics: Output[Metrics],\n",
    "    plots: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"threshold_metric\", float)]):\n",
    "\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve\n",
    "\n",
    "    # Variables\n",
    "    metricsGCSMountPath = metricsUri.replace(\"gs://\", \"/gcs/\")\n",
    "    labels = [\"yes\", \"no\"]\n",
    "\n",
    "    # Helpers\n",
    "    def fnCalculateROC(metrics, true, score):\n",
    "        y_true_np = np.array(metrics[true])\n",
    "        y_score_np = np.array(metrics[score])\n",
    "        fpr, tpr, thresholds = roc_curve(\n",
    "            y_true=y_true_np, y_score=y_score_np, pos_label=True\n",
    "        )\n",
    "        return fpr, tpr, thresholds\n",
    "\n",
    "    def fnCalculateConfusionMatrix(metrics, true, prediction):\n",
    "        y_true_np = np.array(metrics[true])\n",
    "        y_pred_np = np.array(metrics[prediction])\n",
    "        c_matrix = confusion_matrix(y_true_np, y_pred_np)\n",
    "        return c_matrix\n",
    "\n",
    "    # Main\n",
    "    with open(metricsGCSMountPath, mode=\"r\") as json_file:\n",
    "        metricsDictionary = json.load(json_file)\n",
    "\n",
    "    area_roc = metricsDictionary[\"test_area_roc\"]\n",
    "    area_prc = metricsDictionary[\"test_area_prc\"]\n",
    "    acc = metricsDictionary[\"test_accuracy\"]\n",
    "    f1 = metricsDictionary[\"test_f1\"]\n",
    "    prec = metricsDictionary[\"test_precision\"]\n",
    "    rec = metricsDictionary[\"test_recall\"]\n",
    "\n",
    "    metrics.log_metric(\"Test_areaUnderROC\", area_roc)\n",
    "    metrics.log_metric(\"Test_areaUnderPRC\", area_prc)\n",
    "    metrics.log_metric(\"Test_Accuracy\", acc)\n",
    "    metrics.log_metric(\"Test_f1-score\", f1)\n",
    "    metrics.log_metric(\"Test_Precision\", prec)\n",
    "    metrics.log_metric(\"Test_Recall\", rec)\n",
    "\n",
    "    fpr, tpr, thresholds = fnCalculateROC(metricsDictionary, \"true\", \"score\")\n",
    "    c_matrix = fnCalculateConfusionMatrix(metricsDictionary, \"true\", \"prediction\")\n",
    "    plots.log_roc_curve(fpr.tolist(), tpr.tolist(), thresholds.tolist())\n",
    "    plots.log_confusion_matrix(labels, c_matrix.tolist())\n",
    "\n",
    "    componentOutputsTuple = NamedTuple(\n",
    "        \"Outputs\",\n",
    "        [\n",
    "            (\"threshold_metric\", float),\n",
    "        ],\n",
    "    )\n",
    "    return componentOutputsTuple(area_prc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a3c111-17dc-4248-9df8-7c65f73c696c",
   "metadata": {},
   "source": [
    "### 5. Define Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8902ec5e-54ec-4010-8f20-52e8c2ab6a6e",
   "metadata": {},
   "source": [
    "##### Option 1: In this version we dont disable task level caching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346cdd5b-53a4-4760-b4f7-e595f7c20d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NM, \n",
    "    description=\"A SparkMLlib MLOps Vertex pipeline\")\n",
    "def fnSparkMlopsPipeline(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    service_account: str = UMSA_FQN,\n",
    "    subnetwork_uri: str = SUBNET_RESOURCE_URI,\n",
    "    spark_phs_nm: str = PERSISTENT_SPARK_HISTORY_SERVER_RESOURCE_URI,\n",
    "    container_image: str = DOCKER_IMAGE_FQN,\n",
    "    common_utils_py_fqn: list = PYSPARK_COMMON_UTILS_SCRIPT_FQP,\n",
    "    data_preprocessing_pyspark_batch_id: str = DATA_PREPROCESSING_BATCH_INSTANCE_ID,\n",
    "    data_preprocessing_main_py_fqn: str = DATA_PREPROCESSING_MAIN_PY_SCRIPT,\n",
    "    data_preprocessing_args: list = DATA_PREPROCESSING_ARGS,\n",
    "    managed_dataset_display_nm: str = MANAGED_DATASET_NM,\n",
    "    managed_dataset_src_uri: str = DATA_PROCESSING_BQ_SINK_URI,\n",
    "    model_training_pyspark_batch_id: str = MODEL_TRAINING_BATCH_INSTANCE_ID,\n",
    "    model_training_main_py_fqn: str = MODEL_TRAINING_MAIN_PY_SCRIPT,\n",
    "    model_training_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "    model_training_args: list = MODEL_TRAINING_ARGS,\n",
    "    threshold: float = AUPR_THRESHOLD,\n",
    "    hyperparameter_tuning_pyspark_batch_id: str = HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID,\n",
    "    hyperparameter_tuning_main_py_fqn: str = HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT,\n",
    "    hyperparameter_tuning_args: list = HYPERPARAMETER_TUNING_ARGS,\n",
    "    hyperparameter_tuning_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "):\n",
    "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    # Step 1. PRE-PROCESS DATA in PREP FOR MODEL TRAINING\n",
    "    # ....................................................................\n",
    "    preprocessingStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = data_preprocessing_pyspark_batch_id,\n",
    "        main_python_file_uri = data_preprocessing_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = data_preprocessing_args\n",
    "    ).set_display_name(\"Preprocessing\")\n",
    "    \n",
    "    \n",
    "    # Step 2. REGISTER PRE-PROCESSED DATA AS MANAGED DATASET\n",
    "    # ....................................................................\n",
    "    createManagedDatasetStep = vertex_ai_components.TabularDatasetCreateOp(\n",
    "        display_name= managed_dataset_display_nm,\n",
    "        bq_source=managed_dataset_src_uri,\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "    ).after(preprocessingStep).set_display_name(\"Dataset registration\")\n",
    "    \n",
    "    # Step 3. TRAIN MODEL\n",
    "    # .................................................................... \n",
    "    trainSparkMLModelStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = model_training_pyspark_batch_id,\n",
    "        main_python_file_uri = model_training_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = model_training_args\n",
    "    ).after(preprocessingStep).set_display_name(\"Model training\")\n",
    "    \n",
    "    # Step 4. EVALUATE MODEL\n",
    "    # .................................................................... \n",
    "    evaluateModelStep = fnEvaluateModel(model_training_metrics_fqp).after(trainSparkMLModelStep).set_display_name(\"Evaluate model\")\n",
    "    \n",
    "    # Step 5. CONDITIONAL HYPERPARAMETER TUNING\n",
    "    # .................................................................... \n",
    "    with Condition(\n",
    "        evaluateModelStep.outputs[\"threshold_metric\"] >= threshold,\n",
    "        name=\"AUPR Threshold Exceeded\",\n",
    "    ):\n",
    "        # HYPERPARAMETER TUNING\n",
    "        hyperparameterTuningStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = hyperparameter_tuning_pyspark_batch_id,\n",
    "        main_python_file_uri = hyperparameter_tuning_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = hyperparameter_tuning_args\n",
    "        ).after(evaluateModelStep).set_display_name(\"Hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e6167-2446-4595-88ff-8d01880972ad",
   "metadata": {},
   "source": [
    "##### Option 2: In this version we disable task level caching\n",
    "Prefer this for scheduled execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb932bdd-8e1d-4a04-84bd-10bb6d7bd602",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NM, \n",
    "    description=\"A SparkMLlib MLOps Vertex pipeline\")\n",
    "def fnSparkMlopsPipelineWithoutCaching(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    service_account: str = UMSA_FQN,\n",
    "    subnetwork_uri: str = SUBNET_RESOURCE_URI,\n",
    "    spark_phs_nm: str = PERSISTENT_SPARK_HISTORY_SERVER_RESOURCE_URI,\n",
    "    container_image: str = DOCKER_IMAGE_FQN,\n",
    "    common_utils_py_fqn: list = PYSPARK_COMMON_UTILS_SCRIPT_FQP,\n",
    "    data_preprocessing_pyspark_batch_id: str = DATA_PREPROCESSING_BATCH_INSTANCE_ID,\n",
    "    data_preprocessing_main_py_fqn: str = DATA_PREPROCESSING_MAIN_PY_SCRIPT,\n",
    "    data_preprocessing_args: list = DATA_PREPROCESSING_ARGS,\n",
    "    managed_dataset_display_nm: str = MANAGED_DATASET_NM,\n",
    "    managed_dataset_src_uri: str = DATA_PROCESSING_BQ_SINK_URI,\n",
    "    model_training_pyspark_batch_id: str = MODEL_TRAINING_BATCH_INSTANCE_ID,\n",
    "    model_training_main_py_fqn: str = MODEL_TRAINING_MAIN_PY_SCRIPT,\n",
    "    model_training_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "    model_training_args: list = MODEL_TRAINING_ARGS,\n",
    "    threshold: float = AUPR_THRESHOLD,\n",
    "    hyperparameter_tuning_pyspark_batch_id: str = HYPERPARAMETER_TUNING_BATCH_INSTANCE_ID,\n",
    "    hyperparameter_tuning_main_py_fqn: str = HYPERPARAMETER_TUNING_MAIN_PY_SCRIPT,\n",
    "    hyperparameter_tuning_args: list = HYPERPARAMETER_TUNING_ARGS,\n",
    "    hyperparameter_tuning_metrics_fqp: str = MODEL_METRICS_BUCKET_FQP,\n",
    "):\n",
    "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    # Step 1. PRE-PROCESS DATA in PREP FOR MODEL TRAINING\n",
    "    # ....................................................................\n",
    "    preprocessingStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = data_preprocessing_pyspark_batch_id,\n",
    "        main_python_file_uri = data_preprocessing_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = data_preprocessing_args\n",
    "    ).set_caching_options(False).set_display_name(\"Preprocessing\")\n",
    "    \n",
    "    \n",
    "    # Step 2. REGISTER PRE-PROCESSED DATA AS MANAGED DATASET\n",
    "    # ....................................................................\n",
    "    createManagedDatasetStep = vertex_ai_components.TabularDatasetCreateOp(\n",
    "        display_name= managed_dataset_display_nm,\n",
    "        bq_source=managed_dataset_src_uri,\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "    ).after(preprocessingStep).set_caching_options(False).set_display_name(\"Dataset registration\")\n",
    "    \n",
    "    # Step 3. TRAIN MODEL\n",
    "    # .................................................................... \n",
    "    trainSparkMLModelStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = model_training_pyspark_batch_id,\n",
    "        main_python_file_uri = model_training_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = model_training_args\n",
    "    ).set_caching_options(False).after(preprocessingStep).set_display_name(\"Model training\")\n",
    "    \n",
    "    # Step 4. EVALUATE MODEL\n",
    "    # .................................................................... \n",
    "    evaluateModelStep = fnEvaluateModel(model_training_metrics_fqp).after(trainSparkMLModelStep).set_caching_options(False).set_display_name(\"Evaluate model\")\n",
    "    \n",
    "    # Step 5. CONDITIONAL HYPERPARAMETER TUNING\n",
    "    # .................................................................... \n",
    "    with Condition(\n",
    "        evaluateModelStep.outputs[\"threshold_metric\"] >= threshold,\n",
    "        name=\"AUPR Threshold Exceeded\",\n",
    "    ):\n",
    "        # HYPERPARAMETER TUNING\n",
    "        hyperparameterTuningStep = DataprocPySparkBatchOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        container_image = container_image,\n",
    "        subnetwork_uri = subnetwork_uri,\n",
    "        spark_history_dataproc_cluster = spark_phs_nm,\n",
    "        service_account = service_account,     \n",
    "        batch_id = hyperparameter_tuning_pyspark_batch_id,\n",
    "        main_python_file_uri = hyperparameter_tuning_main_py_fqn,\n",
    "        python_file_uris = common_utils_py_fqn,\n",
    "        args = hyperparameter_tuning_args\n",
    "        ).after(evaluateModelStep).set_caching_options(False).set_display_name(\"Hyperparameter tuning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d126d252-8cad-4615-bb65-1126792ef9e7",
   "metadata": {},
   "source": [
    "### 5. Compile the Vertex AI Pipeline into a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ecfbc-dcad-4848-9acf-de6687c91701",
   "metadata": {},
   "outputs": [],
   "source": [
    "if WITHOUT_TASK_CACHING:\n",
    "    compiler.Compiler().compile(pipeline_func=fnSparkMlopsPipelineWithoutCaching, package_path=PIPELINE_PACKAGE_SRC_LOCAL_PATH)\n",
    "    print(\"Executing fnSparkMlopsPipelineWithoutCaching\")\n",
    "else:\n",
    "    compiler.Compiler().compile(pipeline_func=fnSparkMlopsPipeline, package_path=PIPELINE_PACKAGE_SRC_LOCAL_PATH)\n",
    "    print(\"Executing fnSparkMlopsPipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f665f0d-7a95-4dfb-b210-a959a3a3f914",
   "metadata": {},
   "source": [
    "### 6. Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f8be4-5369-4078-8191-96ca42eeb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = vertex_ai.PipelineJob(\n",
    "    display_name=PIPELINE_NM,\n",
    "    template_path=PIPELINE_PACKAGE_SRC_LOCAL_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT_GCS_URI,\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd619ec-ecf7-4318-991a-b9330c1f51e3",
   "metadata": {},
   "source": [
    "### 7. Submit the Pipeline for execution\n",
    "There are two options below, one that uses the customer specified network that is peered with Vertex AI tenant network, and one that uses Vertex AI tenant network altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c96b7f-ab01-4546-9eec-d53aa1800c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BYO_NETWORK:\n",
    "    pipeline.submit(service_account=UMSA_FQN, network=f\"projects/{PROJECT_NBR}/global/networks/{VPC_NM}\")\n",
    "else:\n",
    "    pipeline.submit(service_account=UMSA_FQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b5e66-c87a-40d1-b607-80a40260c07c",
   "metadata": {},
   "source": [
    "### 8. How do we automate this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5de77-129a-411e-89bf-5d34cba0895b",
   "metadata": {},
   "source": [
    "To automate this, we will take the compiled JSON, (1) test run it via Vertex AI Cloud Console UI, and then (2) schedule with Cloud Scheduler (calls Cloud Function that calls the Vertex AI REST endpoint for the Vertex AI pipeline).<br>This is a lab module that you can proceed to next.<br>\n",
    "Note: The json below has your project details. The lab author has a de-identified version with placeholders for your information in the git repo ../04-templates/\\*.json and as part of the Terraform automation, a customized version except custom pipeline ID is placed in 05-pipelines in your local directory in cloud shell. A copy of it is placed in GCS -> s8s-pipelines-bucket-YOUR_PROJECT_NUMBER/templates. At scheduled run time, a new custom pipeline ID is generated in the cloud function, and substituted in the json and placed in the bucket - s8s-pipelines-bucket-YOUR_PROJECT_NUMBER/execution and this is used to launch the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eadd022-cd68-4ab4-9be7-bcf7a2e4f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat $LOCAL_SCRATCH_DIR/*"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
