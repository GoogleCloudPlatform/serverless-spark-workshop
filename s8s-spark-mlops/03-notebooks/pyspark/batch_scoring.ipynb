{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "463506b1-7a4f-49ac-a131-0b5d7561164e",
   "metadata": {},
   "source": [
    "# Batch Scoring\n",
    "\n",
    "This script does batch scoring.\n",
    "1. It parses, transforms data in GCS to be scored, \n",
    "2. Loads the model in GCS \n",
    "(note: you need to update the modelVersion variable with the version number from the model training notebook),\n",
    "3. Uses the model to predict\n",
    "4. Persists predictions to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c46b0-f2e8-4eb4-9a4c-e26ab647caf4",
   "metadata": {},
   "source": [
    "Copyright 2022 Google LLC\n",
    "\n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe3069-3d05-4e94-8954-1997d6511e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fc9b1-36b2-4fd8-9765-7ad15b3d02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml import PipelineModel\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b8dbc-9ea5-4391-870e-cb32f406e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a. Arguments\n",
    "pipelineID = random.randint(1, 10000)\n",
    "projectNbr = \"YOUR_PROJECT_NBR\"\n",
    "projectID = \"YOUR_PROJECT_ID\"\n",
    "modelVersion = pipelineID\n",
    "displayPrintStatements = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b4bc7-4f08-4411-9b1f-540b34d7be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b. Variables \n",
    "appBaseName = \"customer-churn-model\"\n",
    "appNameSuffix = \"batch-scoring\"\n",
    "appName = f\"{appBaseName}-{appNameSuffix}\"\n",
    "modelBaseNm = appBaseName\n",
    "bqDatasetNm = f\"{projectID}.customer_churn_ds\"\n",
    "modelBucketUri = f\"gs://s8s_model_bucket-{projectNbr}/{modelBaseNm}/hyperparameter-tuning/{modelVersion}\"\n",
    "scoreDatasetBucketFQN = f\"gs://s8s_data_bucket-{projectNbr}/customer_churn_score_data.csv\"\n",
    "bigQueryOutputTableFQN = f\"{bqDatasetNm}.batch_predictions\"\n",
    "scratchBucketUri = f\"s8s-spark-bucket-{projectNbr}/{appBaseName}/pipelineId-{pipelineID}/{appNameSuffix}/\"\n",
    "pipelineExecutionDt = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e57cbb-7585-4118-8a90-a6844182374b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1c. Display input and output\n",
    "if displayPrintStatements:\n",
    "    print(\"Starting batch_scoring for Customer Churn Predictions\")\n",
    "    print(\".....................................................\")\n",
    "    print(f\"The datetime now is - {pipelineExecutionDt}\")\n",
    "    print(\" \")\n",
    "    print(\"INPUT-\")\n",
    "    print(f\"....pipelineID={pipelineID}\")\n",
    "    print(f\"....modelVersion={modelVersion}\")\n",
    "    print(f\"....projectNbr={projectNbr}\")\n",
    "    print(f\"....projectID={projectID}\")\n",
    "    print(f\"....displayPrintStatements={displayPrintStatements}\")\n",
    "    print(\" \")\n",
    "    print(\"OUTPUT-\")\n",
    "    print(f\"....BigQuery Table={bigQueryOutputTableFQN}\")\n",
    "    print(f\"SELECT * FROM {bigQueryOutputTableFQN} WHERE model_version='{modelVersion}' AND pipeline_id='{pipelineID}' AND pipeline_execution_dt='{pipelineExecutionDt}' LIMIT 10\" )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822c32c-984d-42e8-acaa-1c64712c1119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Spark Session creation\n",
    "print('....Initializing spark & spark configs')\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "\n",
    "# Spark configuration setting for writes to BigQuery\n",
    "spark.conf.set(\"parentProject\", projectID)\n",
    "spark.conf.set(\"temporaryGcsBucket\", scratchBucketUri)\n",
    "\n",
    "# Add Python modules\n",
    "sc.addPyFile(f\"gs://s8s_code_bucket-{projectNbr}/pyspark/common_utils.py\")\n",
    "import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7259f0f-a5e6-4e5b-876d-fe08de2ba659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Read data to be scored from GCS\n",
    "print('....Read batch scoring input and profile')\n",
    "scoreRawDF = spark.read.options(inferSchema = True, header= True).csv(scoreDatasetBucketFQN)\n",
    "if displayPrintStatements:\n",
    "    print(scoreRawDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bb7db-642b-4412-b4c7-7c2ab3ff7999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Display data, display summary stats\n",
    "if displayPrintStatements:\n",
    "    scoreRawDF.show(2)\n",
    "    scoreRawDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0d470-140b-498b-bd56-054aa7d9198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Replace spaces, space with null values in the TotalCharges and MonthlyCharges columns\n",
    "print('....Data pre-process: fnReplaceSpaceWithNone in TotalCharges and MonthlyCharges')\n",
    "spaceReplacedDF = common_utils.fnReplaceSpaceWithNone(scoreRawDF)\n",
    "if displayPrintStatements:\n",
    "    print(spaceReplacedDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eae237-9f27-48aa-b22f-e360fee251f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Replace non-numeric values in the TotalCharges and MonthlyCharges columns\n",
    "print('....Data pre-process: ReplaceNotANumberWithNone in TotalCharges and MonthlyCharges')\n",
    "nanReplacedDF = common_utils.fnReplaceNotANumberWithNone(spaceReplacedDF)\n",
    "if displayPrintStatements:\n",
    "    print(nanReplacedDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f016816-d830-44ca-9926-c41d2409e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Drop rows with null in columns\n",
    "print('....Data pre-process: Drop rows with none')\n",
    "nullDroppedDF = nanReplacedDF.na.drop()\n",
    "\n",
    "if displayPrintStatements:\n",
    "    print(nullDroppedDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5eff9-7ab0-46a3-bd80-8794582c50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Replace 'No internet service' across columns to 'No'\n",
    "print('....Data pre-process: Replace -No internet service- across columns with -No-')\n",
    "partiallyProcessedDF = common_utils.fnReplaceWithNoForInternetService(nullDroppedDF)\n",
    "if displayPrintStatements:\n",
    "    print(partiallyProcessedDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67583d3b-fd56-45c2-b7da-16158d11a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Add a bin/bucket category for tenure range using Spark SQL and write transformed to dataframe\n",
    "print('....Data pre-process: Create bins by tenure months') \n",
    "scoreTargetDF = common_utils.fnAddBinForTenure(partiallyProcessedDF, True, spark)\n",
    "if displayPrintStatements:\n",
    "    print(scoreTargetDF.count())\n",
    "    scoreTargetDF.show(2)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893d9f1-33dc-434f-b3b6-49c512d3ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Format dataframe names for column name format consistency\n",
    "scorableDF = scoreTargetDF.select(\"customerID\", \"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"tenure\", \"Tenure_Group\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"MonthlyCharges\", \"TotalCharges\") \\\n",
    "                                .toDF(\"customer_id\", \"gender\", \"senior_citizen\", \"partner\", \"dependents\", \"tenure\", \"tenure_group\", \"phone_service\", \"multiple_lines\", \"internet_service\", \"online_security\", \"online_backup\", \"device_protection\", \"tech_support\", \"streaming_tv\", \"streaming_movies\", \"contract\", \"paperless_billing\", \"payment_method\", \"monthly_charges\", \"total_charges\") \n",
    "\n",
    "if displayPrintStatements:\n",
    "    print(scorableDF.count())\n",
    "    scorableDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8316cb-0df2-4667-98a8-60c4a1428f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Load the pre-trained, persisted model in GCS\n",
    "print('....Scoring: Load model out of GCS into memory') \n",
    "model = PipelineModel.load(f\"{modelBucketUri}/bestModel/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36f456-7bea-4b77-b7c2-534e6be5379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Batch scoring\n",
    "print('....Scoring: Execute model.transform') \n",
    "batchScoreResultsDF = model.transform(scorableDF) \\\n",
    "                           .withColumn(\"model_version\", lit(modelVersion).cast(\"string\")) \\\n",
    "                           .withColumn(\"pipeline_id\", lit(pipelineID).cast(\"string\")) \\\n",
    "                           .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \n",
    "\n",
    "if displayPrintStatements:\n",
    "    batchScoreResultsDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475788b-62ca-4f4d-bf6f-b1c1330f3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Persist to BigQuery\n",
    "print('....Persisting: Batch scoring results to BigQuery')\n",
    "batchScoreResultsDF.select(\"customer_id\", \"gender\", \"senior_citizen\", \"partner\", \"dependents\", \"tenure\", \"tenure_group\", \"phone_service\", \"multiple_lines\", \"internet_service\", \"online_security\", \"online_backup\", \"device_protection\", \"tech_support\", \"streaming_tv\", \"streaming_movies\", \"contract\", \"paperless_billing\", \"payment_method\", \"monthly_charges\", \"total_charges\",\"prediction\",\"model_version\",\"pipeline_id\",\"pipeline_execution_dt\") \\\n",
    ".write.format('bigquery') \\\n",
    ".mode(\"append\")\\\n",
    ".option('table', bigQueryOutputTableFQN) \\\n",
    ".save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "serverless_spark": "{\"name\":\"projects/s8s-spark-ml-mlops/locations/us-central1/sessions/vai-2\",\"uuid\":\"8c53d840-5a00-4c72-bf58-413be0fb4d1d\",\"createTime\":\"2022-08-04T03:41:17.312074Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{},\"state\":\"ACTIVE\",\"stateTime\":\"2022-08-04T03:42:28.412751Z\",\"creator\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"runtimeConfig\":{\"containerImage\":\"gcr.io/s8s-spark-ml-mlops/dataproc_serverless_custom_runtime:1.0.3\",\"properties\":{\"spark:spark.executor.cores\":\"8\",\"spark:spark.executor.instances\":\"5\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.eventLog.dir\":\"gs://s8s-sphs-974925525028/8c53d840-5a00-4c72-bf58-413be0fb4d1d/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"subnetworkUri\":\"https://www.googleapis.com/compute/v1/projects/s8s-spark-ml-mlops/regions/us-central1/subnetworks/spark-snet\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/s8s-spark-ml-mlops/regions/us-central1/clusters/s8s-sphs-974925525028\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-08-04T03:41:17.312074Z\"}]}",
  "serverless_spark_kernel_name": "remote-7c7e15bf0de294d98b312d6c-pyspark"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
