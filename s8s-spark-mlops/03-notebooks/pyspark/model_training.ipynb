{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING\n",
    "This script does training with Spark MLLib of a Random Forest Classification model for the customer churn prediction experiment-</br>\n",
    "Uses BigQuery as a source, and writes test results, model metrics and \n",
    "feature importance scores to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2022 Google LLC\n",
    "\n",
    " Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    " you may not use this file except in compliance with the License.\n",
    " You may obtain a copy of the License at\n",
    "\n",
    "      http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    " Unless required by applicable law or agreed to in writing, software\n",
    " distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " See the License for the specific language governing permissions and\n",
    " limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import pandas as pd\n",
    "import sys, logging, argparse, random, tempfile, json\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "from pyspark.sql.types import StructType, DoubleType, StringType\n",
    "from pyspark.sql.functions import lit\n",
    "from pathlib import Path as path\n",
    "from google.cloud import storage\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a. Arguments\n",
    "pipelineID = random.randint(1, 10000)\n",
    "projectNbr = \"YOUR_PROJECT_NBR\"\n",
    "projectID = \"YOUR_PROJECT_ID\"\n",
    "displayPrintStatements = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b. Variables \n",
    "appBaseName = \"customer-churn-model\"\n",
    "appNameSuffix = \"training\"\n",
    "appName = f\"{appBaseName}-{appNameSuffix}\"\n",
    "modelBaseNm = appBaseName\n",
    "modelVersion = pipelineID\n",
    "bqDatasetNm = f\"{projectID}.customer_churn_ds\"\n",
    "operation = appNameSuffix\n",
    "bigQuerySourceTableFQN = f\"{bqDatasetNm}.training_data\"\n",
    "bigQueryModelTestResultsTableFQN = f\"{bqDatasetNm}.test_predictions\"\n",
    "bigQueryModelMetricsTableFQN = f\"{bqDatasetNm}.model_metrics\"\n",
    "bigQueryFeatureImportanceTableFQN = f\"{bqDatasetNm}.model_feature_importance_scores\"\n",
    "modelBucketUri = f\"gs://s8s_model_bucket-{projectNbr}/{modelBaseNm}/{operation}/{modelVersion}\"\n",
    "metricsBucketUri = f\"gs://s8s_metrics_bucket-{projectNbr}/{modelBaseNm}/{operation}/{modelVersion}\"\n",
    "scratchBucketUri = f\"s8s-spark-bucket-{projectNbr}/{appBaseName}/pipelineId-{pipelineID}/{appNameSuffix}/\"\n",
    "pipelineExecutionDt = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other variables, constants\n",
    "SPLIT_SEED = 6\n",
    "SPLIT_SPECS = [0.8, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1c. Display input and output\n",
    "if displayPrintStatements:\n",
    "    print(\"Starting model training for *Customer Churn* experiment\")\n",
    "    print(\".....................................................\")\n",
    "    print(f\"The datetime now is - {pipelineExecutionDt}\")\n",
    "    print(\" \")\n",
    "    print(\"INPUT PARAMETERS\")\n",
    "    print(f\"....pipelineID={pipelineID}\")\n",
    "    print(f\"....projectID={projectID}\")\n",
    "    print(f\"....projectNbr={projectNbr}\")\n",
    "    print(f\"....displayPrintStatements={displayPrintStatements}\")\n",
    "    print(\" \")\n",
    "    print(\"EXPECTED SETUP\")  \n",
    "    print(f\"....BQ Dataset={bqDatasetNm}\")\n",
    "    print(f\"....Model Training Source Data in BigQuery={bigQuerySourceTableFQN}\")\n",
    "    print(f\"....Scratch Bucket for BQ connector=gs://s8s-spark-bucket-{projectNbr}\") \n",
    "    print(f\"....Model Bucket=gs://s8s-model-bucket-{projectNbr}\")  \n",
    "    print(f\"....Metrics Bucket=gs://s8s-metrics-bucket-{projectNbr}\") \n",
    "    print(\" \")\n",
    "    print(\"OUTPUT\")\n",
    "    print(f\"....Model in GCS={modelBucketUri}\")\n",
    "    print(f\"....Model metrics in GCS={metricsBucketUri}\")  \n",
    "    print(f\"....Model metrics in BigQuery={bigQueryModelMetricsTableFQN}\")      \n",
    "    print(f\"....Model feature importance scores in BigQuery={bigQueryFeatureImportanceTableFQN}\") \n",
    "    print(f\"....Model test results in BigQuery={bigQueryModelTestResultsTableFQN}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Spark Session creation\n",
    "print('....Initializing spark & spark configs')\n",
    "spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "\n",
    "# Spark configuration setting for writes to BigQuery\n",
    "spark.conf.set(\"parentProject\", projectID)\n",
    "spark.conf.set(\"temporaryGcsBucket\", scratchBucketUri)\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Add Python modules\n",
    "sc.addPyFile(f\"gs://s8s_code_bucket-{projectNbr}/pyspark/common_utils.py\")\n",
    "import common_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING DATA - READ, SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Read training data\n",
    "print('....Read the training dataset into a dataframe')\n",
    "inputDF = spark.read \\\n",
    "    .format('bigquery') \\\n",
    "    .load(bigQuerySourceTableFQN)\n",
    "\n",
    "inputDF.printSchema()\n",
    "\n",
    "if displayPrintStatements:\n",
    "    print(f\"inputDF count={inputDF.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typecast some columns to the right datatype\n",
    "inputDF = inputDF.withColumn(\"partner\", inputDF.partner.cast('string')) \\\n",
    "    .withColumn(\"dependents\", inputDF.dependents.cast('string')) \\\n",
    "    .withColumn(\"phone_service\", inputDF.phone_service.cast('string')) \\\n",
    "    .withColumn(\"paperless_billing\", inputDF.paperless_billing.cast('string')) \\\n",
    "    .withColumn(\"churn\", inputDF.churn.cast('string')) \\\n",
    "    .withColumn(\"monthly_charges\", inputDF.monthly_charges.cast('float')) \\\n",
    "    .withColumn(\"total_charges\", inputDF.total_charges.cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split to training and test datasets\n",
    "print('....Split the dataset')\n",
    "trainDF, testDF = inputDF.randomSplit(SPLIT_SPECS, seed=SPLIT_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING & FEATURE ENGINEERING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Pre-process training data\n",
    "print('....Data pre-procesing')\n",
    "dataPreprocessingStagesList = []\n",
    "# 5a. Create and append to pipeline stages - string indexing and one hot encoding\n",
    "for eachCategoricalColumn in common_utils.CATEGORICAL_COLUMN_LIST:\n",
    "    # Category indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=eachCategoricalColumn, outputCol=eachCategoricalColumn + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[eachCategoricalColumn + \"classVec\"])\n",
    "    # Add stages.  This is a lazy operation\n",
    "    dataPreprocessingStagesList += [stringIndexer, encoder]\n",
    "\n",
    "# 5b. Convert label into label indices using the StringIndexer and append to pipeline stages\n",
    "labelStringIndexer = StringIndexer(inputCol=\"churn\", outputCol=\"label\")\n",
    "dataPreprocessingStagesList += [labelStringIndexer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature engineering\n",
    "print('....Feature engineering')\n",
    "featureEngineeringStageList = []\n",
    "assemblerInputs = common_utils.NUMERIC_COLUMN_LIST + [c + \"classVec\" for c in common_utils.CATEGORICAL_COLUMN_LIST]\n",
    "featuresVectorAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "featureEngineeringStageList += [featuresVectorAssembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model training\n",
    "print('....Model training')\n",
    "modelTrainingStageList = []\n",
    "rfClassifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "modelTrainingStageList += [rfClassifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create a model training pipeline for stages defined\n",
    "print('....Instantiating pipeline model')\n",
    "pipeline = Pipeline(stages=dataPreprocessingStagesList + featureEngineeringStageList + modelTrainingStageList) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# 9. Fit the model\n",
    "print('....Fit the model')\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Test the model with the test dataset\n",
    "print('....Test the model')\n",
    "predictionsDF = pipelineModel.transform(testDF)\n",
    "predictionsDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# 11. Persist model to GCS\n",
    "print('....Persist the model to GCS')\n",
    "pipelineModel.write().overwrite().save(modelBucketUri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Persist model testing results to BigQuery\n",
    "persistPredictionsDF = predictionsDF.withColumn(\"pipeline_id\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"model_version\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \\\n",
    "                                   .withColumn(\"operation\", lit(appNameSuffix)) \n",
    "\n",
    "persistPredictionsDF.write.format('bigquery') \\\n",
    ".mode(\"append\")\\\n",
    ".option('table', bigQueryModelTestResultsTableFQN) \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EXPLAINABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13a. Model explainability - feature importance\n",
    "pipelineModel.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13b. Function to parse feature importance\n",
    "def fnExtractFeatureImportance(featureImportanceSparseVector, predictionsDataframe, featureColumnListing):\n",
    "    featureColumnMetadataList = []\n",
    "    for i in predictionsDataframe.schema[featureColumnListing].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        featureColumnMetadataList = featureColumnMetadataList + predictionsDataframe.schema[featureColumnListing].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "        \n",
    "    featureColumnMetadataPDF = pd.DataFrame(featureColumnMetadataList)\n",
    "    featureColumnMetadataPDF['importance_score'] = featureColumnMetadataPDF['idx'].apply(lambda x: featureImportanceSparseVector[x])\n",
    "    return(featureColumnMetadataPDF.sort_values('importance_score', ascending = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13c. Print feature importance\n",
    "fnExtractFeatureImportance(pipelineModel.stages[-1].featureImportances, predictionsDF, \"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13d. Capture into a Pandas DF\n",
    "featureImportantcePDF = fnExtractFeatureImportance(pipelineModel.stages[-1].featureImportances, predictionsDF, \"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 212:>                                                        (0 + 8) / 8]\r"
     ]
    }
   ],
   "source": [
    "# 13e. Persist feature importance scores to BigQuery\n",
    "# Convert Pandas to Spark DF & use Spark to persist\n",
    "featureImportantceDF = spark.createDataFrame(featureImportantcePDF).toDF(\"feature_index\",\"feature_nm\",\"importance_score\")\n",
    "\n",
    "persistFeatureImportanceDF = featureImportantceDF.withColumn(\"pipeline_id\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"model_version\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \\\n",
    "                                   .withColumn(\"operation\", lit(operation)) \n",
    "\n",
    "persistFeatureImportanceDF.show(2)\n",
    "\n",
    "persistFeatureImportanceDF.write.format('bigquery') \\\n",
    ".mode(\"append\")\\\n",
    ".option('table', bigQueryFeatureImportanceTableFQN) \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14a. Metrics parsing function\n",
    "def fnParseModelMetrics(predictionsDF, labelColumn, operation, boolSubsetOnly):\n",
    "    \"\"\"\n",
    "    Get model metrics\n",
    "    Args:\n",
    "        predictions: predictions\n",
    "        labelColumn: target column\n",
    "        operation: train or test\n",
    "        boolSubsetOnly: boolean for partial(without true, score, prediction) or full metrics \n",
    "    Returns:\n",
    "        metrics: metrics\n",
    "        \n",
    "    Anagha TODO: This function if called from common_utils fails; Need to researchy why\n",
    "    \"\"\"\n",
    "    \n",
    "    metricLabels = ['area_roc', 'area_prc', 'accuracy', 'f1', 'precision', 'recall']\n",
    "    metricColumns = ['true', 'score', 'prediction']\n",
    "    metricKeys = [f'{operation}_{ml}' for ml in metricLabels] + metricColumns\n",
    "\n",
    "    # Instantiate evaluators\n",
    "    bcEvaluator = BinaryClassificationEvaluator(labelCol=labelColumn)\n",
    "    mcEvaluator = MulticlassClassificationEvaluator(labelCol=labelColumn)\n",
    "\n",
    "    # Capture metrics -> areas, acc, f1, prec, rec\n",
    "    area_roc = round(bcEvaluator.evaluate(predictionsDF, {bcEvaluator.metricName: 'areaUnderROC'}), 5)\n",
    "    area_prc = round(bcEvaluator.evaluate(predictionsDF, {bcEvaluator.metricName: 'areaUnderPR'}), 5)\n",
    "    acc = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"accuracy\"}), 5)\n",
    "    f1 = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"f1\"}), 5)\n",
    "    prec = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"weightedPrecision\"}), 5)\n",
    "    rec = round(mcEvaluator.evaluate(predictionsDF, {mcEvaluator.metricName: \"weightedRecall\"}), 5)\n",
    "\n",
    "    # Get the true, score, prediction off of the test results dataframe\n",
    "    rocDictionary = common_utils.fnGetTrueScoreAndPrediction(predictionsDF, labelColumn)\n",
    "    true = rocDictionary['true']\n",
    "    score = rocDictionary['score']\n",
    "    prediction = rocDictionary['prediction']\n",
    "\n",
    "    # Create a metric values array\n",
    "    metricValuesArray = []\n",
    "    if boolSubsetOnly:\n",
    "        metricValuesArray.extend((area_roc, area_prc, acc, f1, prec, rec))\n",
    "    else:\n",
    "        metricValuesArray.extend((area_roc, area_prc, acc, f1, prec, rec, true, score, prediction))\n",
    "    \n",
    "    # Zip the keys and values into a dictionary  \n",
    "    metricsDictionary = dict(zip(metricKeys, metricValuesArray))\n",
    "\n",
    "    return metricsDictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14b. Capture & display metrics\n",
    "modelMetrics = fnParseModelMetrics(predictionsDF, \"label\", \"test\", True)\n",
    "for m, v in modelMetrics.items():\n",
    "    print(f'{m}: {v}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14c. Persist metrics subset to GCS\n",
    "blobName = f\"{modelBaseNm}/{operation}/{modelVersion}/subset/metrics.json\"\n",
    "common_utils.fnPersistMetrics(urlparse(metricsBucketUri).netloc, modelMetrics, blobName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14d. Persist metrics in full to GCS\n",
    "# (The version persisted to BQ does not have True, Score and Prediction needed for Confusion Matrix\n",
    "# This version below has the True, Score and Prediction additionally) \n",
    "\n",
    "# 14d.1. Capture\n",
    "modelMetricsWithTSP = fnParseModelMetrics(predictionsDF, \"label\", \"test\", False)\n",
    "\n",
    "# 14d.2. Persist\n",
    "blobName = f\"{modelBaseNm}/{operation}/{modelVersion}/full/metrics.json\"\n",
    "print(blobName)\n",
    "common_utils.fnPersistMetrics(urlparse(metricsBucketUri).netloc, modelMetricsWithTSP, blobName)\n",
    "\n",
    "# 14d.3. Print\n",
    "for m, v in modelMetricsWithTSP.items():\n",
    "    print(f'{m}: {v}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 278:==============>                                          (2 + 6) / 8]\r"
     ]
    }
   ],
   "source": [
    "# 14e. Persist metrics subset to BigQuery\n",
    "metricsDF = spark.createDataFrame(modelMetrics.items(), [\"metric_nm\", \"metric_value\"]) \n",
    "metricsWithPipelineIdDF = metricsDF.withColumn(\"pipeline_id\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"model_version\", lit(pipelineID).cast(\"string\")) \\\n",
    "                                   .withColumn(\"pipeline_execution_dt\", lit(pipelineExecutionDt)) \\\n",
    "                                   .withColumn(\"operation\", lit(operation)) \n",
    "\n",
    "metricsWithPipelineIdDF.show()\n",
    "\n",
    "metricsWithPipelineIdDF.write.format('bigquery') \\\n",
    ".mode(\"append\")\\\n",
    ".option('table', bigQueryModelMetricsTableFQN) \\\n",
    ".save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "serverless_spark": "{\"name\":\"projects/s8s-spark-ml-mlops/locations/us-central1/sessions/agni-6\",\"uuid\":\"35fda7e3-be7b-4913-99c5-83e97b677386\",\"createTime\":\"2022-08-04T02:37:17.836903Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{},\"state\":\"ACTIVE\",\"stateTime\":\"2022-08-04T02:38:37.084371Z\",\"creator\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"runtimeConfig\":{\"containerImage\":\"gcr.io/s8s-spark-ml-mlops/dataproc_serverless_custom_runtime:1.0.3\",\"properties\":{\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.eventLog.dir\":\"gs://s8s-sphs-974925525028/35fda7e3-be7b-4913-99c5-83e97b677386/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"s8s-lab-sa@s8s-spark-ml-mlops.iam.gserviceaccount.com\",\"subnetworkUri\":\"https://www.googleapis.com/compute/v1/projects/s8s-spark-ml-mlops/regions/us-central1/subnetworks/spark-snet\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/s8s-spark-ml-mlops/regions/us-central1/clusters/s8s-sphs-974925525028\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2022-08-04T02:37:17.836903Z\"}]}",
  "serverless_spark_kernel_name": "remote-bc514a4a91cec988ad3c15a7-pyspark"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
