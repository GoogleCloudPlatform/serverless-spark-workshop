{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder as LE\n",
    "import sys\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('churn model') \\\n",
    "                    .getOrCreate()\n",
    " \n",
    "#Reading the arguments and storing them in variables\n",
    "project_name=sys.argv[1]\n",
    "dataset_name=sys.argv[2]\n",
    "bucket_name=sys.argv[3]\n",
    "user_name=sys.argv[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Data into Spark Dataframe.\n",
    "#sparkDF = spark.read \\\n",
    "  #.format('bigquery') \\\n",
    "  #.load(project_name+'.'+dataset_name+'.'+ user_name+'_churn_dat   a')\n",
    "  \n",
    "churn_dataset_df = spark.read.options(inferSchema = True, header= True).csv('gs://'+bucket_name+'/customer-churn-prediction-vertex-ai/01-datasets/customer_churn_train_data.csv')\n",
    "\n",
    "\n",
    "#print(sparkDF)\n",
    "\n",
    "#Data Wrangling\n",
    "#Replacing spaces with null values in total charges column\n",
    "from pyspark.sql.functions import *\n",
    "dfWithEmptyReplaced = churn_dataset_df.withColumn('TotalCharges', when(col('TotalCharges') == ' ', None).otherwise(col('TotalCharges')).cast(\"float\"))\n",
    "dfWithEmptyReplaced = dfWithEmptyReplaced.na.drop()\n",
    "#Replacing 'No internet service' to No for the following columns\n",
    "replace_cols = [ 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                'TechSupport','StreamingTV', 'StreamingMovies']\n",
    "#replace values\n",
    "for col_name in replace_cols:\n",
    "    dfwithNo = dfWithEmptyReplaced.withColumn(col_name, when(col(col_name)== \"No internet service\",\"No\").otherwise(col(col_name)))\n",
    "    dfWithEmptyReplaced = dfwithNo\n",
    "\n",
    "dfwithNo.createOrReplaceTempView(\"datawrangling\")\n",
    "# Using Spark SQL to create categories\n",
    "df_wrangling = spark.sql(\"\"\"\n",
    "select distinct\n",
    "         customerID\n",
    "        ,gender\n",
    "        ,SeniorCitizen\n",
    "        ,Partner\n",
    "        ,Dependents\n",
    "        ,tenure\n",
    "        ,case when (tenure<=12) then \"Tenure_0-12\"\n",
    "              when (tenure>12 and tenure <=24) then \"Tenure_12-24\"\n",
    "              when (tenure>24 and tenure <=48) then \"Tenure_24-48\"\n",
    "              when (tenure>48 and tenure <=60) then \"Tenure_48-60\"\n",
    "              when (tenure>60) then \"Tenure_gt_60\"\n",
    "        end as tenure_group\n",
    "        ,PhoneService\n",
    "        ,MultipleLines\n",
    "        ,InternetService\n",
    "        ,OnlineSecurity\n",
    "        ,OnlineBackup\n",
    "        ,DeviceProtection\n",
    "        ,TechSupport\n",
    "        ,StreamingTV\n",
    "        ,StreamingMovies\n",
    "        ,Contract\n",
    "        ,PaperlessBilling\n",
    "        ,PaymentMethod\n",
    "        ,MonthlyCharges\n",
    "        ,TotalCharges\n",
    "        ,Churn\n",
    "    from datawrangling\n",
    "\"\"\")\n",
    "\n",
    "(trainingData, testData) = df_wrangling.randomSplit([0.7, 0.3], seed=200)\n",
    "spark.conf.set(\"parentProject\", project_name)\n",
    "bucket = bucket_name+\"/customer-churn-prediction-vertex-ai\"\n",
    "spark.conf.set(\"temporaryGcsBucket\",bucket)\n",
    "trainingData.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', project_name+':'+dataset_name+'.'+user_name+'_training_data') \\\n",
    ".save()\n",
    "\n",
    "testData.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', project_name+':'+dataset_name+'.'+user_name+'_test_data') \\\n",
    ".save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark [conda env:root] * (Local)",
   "language": "python",
   "name": "local-conda-root-pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "serverless_spark": "{\"name\":\"projects/tgs-internal-gcpgtminit-dev-01/locations/us-central1/sessions/kkvertex-session\",\"uuid\":\"8e15d845-a08f-466e-9d21-c2103cb43fa4\",\"createTime\":\"2022-05-11T06:20:48.854599Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{\"endpoints\":{\"Spark History Server\":\"https://di4wwkrkine4zkd3ekrinmlc4q-dot-us-central1.dataproc.googleusercontent.com/sparkhistory/\"},\"outputUri\":\"https://ixd634dxyvay7iwlnyfx6h5ek4-dot-us-central1.dataproc.googleusercontent.com/gateway/default/jupyter/lab/\"},\"state\":\"ACTIVE\",\"stateTime\":\"2022-05-11T06:21:46.781294Z\",\"creator\":\"198454710197-compute@developer.gserviceaccount.com\",\"runtimeConfig\":{\"properties\":{\"spark:spark.jars\":\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar\",\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.eventLog.dir\":\"gs://bq_serverless_spark/phs/8e15d845-a08f-466e-9d21-c2103cb43fa4/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"subnetworkUri\":\"subnet-covid\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/tgs-internal-gcpgtminit-dev-01/regions/us-central1/clusters/spark-phs\"}}}}",
  "serverless_spark_kernel_name": "remote-3bf6a892678aed767accba03-pyspark"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
