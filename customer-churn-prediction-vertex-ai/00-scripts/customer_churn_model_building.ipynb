{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e25e65-c0cf-4b40-ad8e-a20f9073f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression,\\\n",
    "                    RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import sys\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('Creating Model') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "#Reading the arguments and storing them in variables\n",
    "project_name=sys.argv[1]\n",
    "dataset_name=sys.argv[2]\n",
    "bucket_name=sys.argv[3]\n",
    "user_name=sys.argv[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7184ea-0369-4f75-8152-f73b2f4e846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = spark.read \\\n",
    "  .format('bigquery') \\\n",
    "  .load(project_name+'.'+dataset_name+'.'+user_name+'_training_data')\n",
    "\n",
    "\n",
    "trainingData=trainingData.withColumn(\"Partner\",trainingData.Partner.cast('string')).withColumn(\"Dependents\",trainingData.Dependents.cast('string')).withColumn(\"PhoneService\",trainingData.PhoneService.cast('string')).withColumn(\"PaperlessBilling\",trainingData.PaperlessBilling.cast('string')).withColumn(\"Churn\",trainingData.Churn.cast('string'))\n",
    "\n",
    "testData = spark.read \\\n",
    "  .format('bigquery') \\\n",
    "  .load(project_name+'.'+dataset_name+'.'+user_name+'_test_data')\n",
    "\n",
    "testData=testData.withColumn(\"Partner\",testData.Partner.cast('string')).withColumn(\"Dependents\",testData.Dependents.cast('string')).withColumn(\"PhoneService\",testData.PhoneService.cast('string')).withColumn(\"PaperlessBilling\",testData.PaperlessBilling.cast('string')).withColumn(\"Churn\",testData.Churn.cast('string'))\n",
    "\n",
    "\n",
    "categoricalColumns = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract','PaperlessBilling','PaymentMethod']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]\n",
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = ['MonthlyCharges', 'TotalCharges']#'TotalRmbRCN1',\n",
    "assemblerInputs = numericCols + [c + \"classVec\" for c in categoricalColumns]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "IDcols = ['customerID']\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label',\n",
    "                                          metricName='accuracy')\n",
    "\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "stages +=[rf]\n",
    "pipeline_rf = Pipeline(stages=stages)\n",
    "\n",
    "\n",
    "rf_model = pipeline_rf.fit(trainingData)\n",
    "\n",
    "test_pred=rf_model.transform(testData)\n",
    "\n",
    "accurac=evaluator.evaluate(test_pred)\n",
    "\n",
    "print(accurac)\n",
    "\n",
    "spark.conf.set(\"parentProject\", project_name)\n",
    "bucket = bucket_name\n",
    "spark.conf.set(\"temporaryGcsBucket\",bucket)\n",
    "test_pred.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', project_name+':'+dataset_name+'.'+user_name+'_predictions_data') \\\n",
    ".save()\n",
    "\n",
    "rf_model.write().overwrite().save('gs://'+bucket_name+'/customer-churn-prediction-vertex-ai/'+user_name+'_churn_model/model_files')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark [conda env:root] * (Local)",
   "language": "python",
   "name": "local-conda-root-pyspark"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
