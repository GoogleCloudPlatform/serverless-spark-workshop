{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d9ecf7-ebbf-4cd2-b3d1-469c02ea83d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright 2023 Google LLC.\n",
    "#SPDX-License-Identifier: Apache-2.0\n",
    "#Importing libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder as LE\n",
    "import sys\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression,\\\n",
    "                    RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a06ec5-b228-4cff-83f4-5c76048e26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('churn model') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835ef02-7746-4569-9ebc-fc91cb59107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the arguments and storing them in variables\n",
    "project_name=<<your_project_name>>\n",
    "dataset_name=<<your_bq_dataset_name>>\n",
    "bucket_name=<<your_code_bucket>>\n",
    "user_name=<<your_username_here>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490b866-9fe2-4311-8ad4-f676af16eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Data into Spark Dataframe\n",
    "churn_dataset_df = spark.read.options(inferSchema = True, header= True).csv('gs://'+bucket_name+'/customer-churn-prediction-vertex-ai/01-datasets/customer_churn_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6985032-3487-4889-b430-1a43c9384fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing spaces with null values in total charges column\n",
    "from pyspark.sql.functions import *\n",
    "dfWithEmptyReplaced = churn_dataset_df.withColumn('TotalCharges', when(col('TotalCharges') == ' ', None).otherwise(col('TotalCharges')).cast(\"float\"))\n",
    "dfWithEmptyReplaced = dfWithEmptyReplaced.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d829e19-178c-4f20-a5d8-eda2f11715d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing 'No internet service' to No for the following columns\n",
    "replace_cols = [ 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                'TechSupport','StreamingTV', 'StreamingMovies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31996e-6ade-4694-a634-58af2c4c5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace values\n",
    "for col_name in replace_cols:\n",
    "    dfwithNo = dfWithEmptyReplaced.withColumn(col_name, when(col(col_name)== \"No internet service\",\"No\").otherwise(col(col_name)))\n",
    "    dfWithEmptyReplaced = dfwithNo\n",
    "dfwithNo.createOrReplaceTempView(\"datawrangling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1efc0d-a781-49e5-8fa6-c1f395ed7224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Spark SQL to create categories\n",
    "df_wrangling = spark.sql(\"\"\"\n",
    "select distinct\n",
    "         customerID\n",
    "        ,gender\n",
    "        ,SeniorCitizen\n",
    "        ,Partner\n",
    "        ,Dependents\n",
    "        ,tenure\n",
    "        ,case when (tenure<=12) then \"Tenure_0-12\"\n",
    "              when (tenure>12 and tenure <=24) then \"Tenure_12-24\"\n",
    "              when (tenure>24 and tenure <=48) then \"Tenure_24-48\"\n",
    "              when (tenure>48 and tenure <=60) then \"Tenure_48-60\"\n",
    "              when (tenure>60) then \"Tenure_gt_60\"\n",
    "        end as tenure_group\n",
    "        ,PhoneService\n",
    "        ,MultipleLines\n",
    "        ,InternetService\n",
    "        ,OnlineSecurity\n",
    "        ,OnlineBackup\n",
    "        ,DeviceProtection\n",
    "        ,TechSupport\n",
    "        ,StreamingTV\n",
    "        ,StreamingMovies\n",
    "        ,Contract\n",
    "        ,PaperlessBilling\n",
    "        ,PaymentMethod\n",
    "        ,MonthlyCharges\n",
    "        ,TotalCharges\n",
    "        ,Churn\n",
    "    from datawrangling\n",
    "\"\"\")\n",
    "\n",
    "(trainingData, testData) = df_wrangling.randomSplit([0.7, 0.3], seed=200)\n",
    "spark.conf.set(\"parentProject\", project_name)\n",
    "bucket = bucket_name+\"/customer-churn-prediction-vertex-ai\"\n",
    "spark.conf.set(\"temporaryGcsBucket\",bucket)\n",
    "trainingData.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', project_name+':'+dataset_name+'.'+user_name+'_training_data') \\\n",
    ".save()\n",
    "\n",
    "testData.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', project_name+':'+dataset_name+'.'+user_name+'_test_data') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a844667-2362-47ea-ab5b-94388ace4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "trainingData = spark.read \\\n",
    "  .format('bigquery') \\\n",
    "  .load(project_name+'.'+dataset_name+'.'+user_name+'_training_data')\n",
    "\n",
    "\n",
    "trainingData=trainingData.withColumn(\"Partner\",trainingData.Partner.cast('string')).withColumn(\"Dependents\",trainingData.Dependents.cast('string')).withColumn(\"PhoneService\",trainingData.PhoneService.cast('string')).withColumn(\"PaperlessBilling\",trainingData.PaperlessBilling.cast('string')).withColumn(\"Churn\",trainingData.Churn.cast('string'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bfe8f8-9ec6-4217-8c5b-1b46071cdaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = spark.read \\\n",
    "  .format('bigquery') \\\n",
    "  .load(project_name+'.'+dataset_name+'.'+user_name+'_test_data')\n",
    "\n",
    "testData=testData.withColumn(\"Partner\",testData.Partner.cast('string')).withColumn(\"Dependents\",testData.Dependents.cast('string')).withColumn(\"PhoneService\",testData.PhoneService.cast('string')).withColumn(\"PaperlessBilling\",testData.PaperlessBilling.cast('string')).withColumn(\"Churn\",testData.Churn.cast('string'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58849508-7b65-4b12-a23a-1b2e3efc0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalColumns = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract','PaperlessBilling','PaymentMethod']\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c24df0-4000-4d8b-a85d-59693926dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"Churn\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c1b06-f3d8-4a7b-811c-1a8f6a3e57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = ['MonthlyCharges', 'TotalCharges']#'TotalRmbRCN1',\n",
    "assemblerInputs = numericCols + [c + \"classVec\" for c in categoricalColumns]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "IDcols = ['customerID']\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label',\n",
    "                                          metricName='accuracy')\n",
    "\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "stages +=[rf]\n",
    "pipeline_rf = Pipeline(stages=stages)\n",
    "\n",
    "\n",
    "rf_model = pipeline_rf.fit(trainingData)\n",
    "\n",
    "test_pred=rf_model.transform(testData)\n",
    "\n",
    "accurac=evaluator.evaluate(test_pred)\n",
    "\n",
    "print(accurac)\n",
    "\n",
    "spark.conf.set(\"parentProject\", project_name)\n",
    "bucket = bucket_name\n",
    "spark.conf.set(\"temporaryGcsBucket\",bucket)\n",
    "test_pred.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', project_name+':'+dataset_name+'.'+user_name+'_predictions_data') \\\n",
    ".save()\n",
    "\n",
    "rf_model.write().overwrite().save('gs://'+bucket_name+'/customer-churn-prediction-vertex-ai/'+user_name+'_churn_model/model_files')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1625e6-b4ef-4beb-b6b2-f8628139593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "sparkDF = spark.read.options(inferSchema = True, header= True).csv('gs://'+bucket_name+'/customer-churn-prediction-vertex-ai/01-datasets/customer_churn_test_model_data.csv')\n",
    "\n",
    "\n",
    "sparkDF=sparkDF.withColumn(\"Partner\",sparkDF.Partner.cast('string')).withColumn(\"Dependents\",sparkDF.Dependents.cast('string')).withColumn(\"PhoneService\",sparkDF.PhoneService.cast('string')).withColumn(\"PaperlessBilling\",sparkDF.PaperlessBilling.cast('string'))\n",
    "sparkDF=sparkDF.head(1)\n",
    "sparkDF=spark.createDataFrame(sparkDF)\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "rf_model = PipelineModel.load(os.path.join('gs://'+bucket_name+'/customer-churn-prediction-vertex-ai/'+user_name+'_churn_model/model_files'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e4086-53c8-4fc2-ad6f-601c0d03b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing 'No internet service' to No for the following columns\n",
    "replace_cols = [ 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                'TechSupport','StreamingTV', 'StreamingMovies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e13172-eb1b-4f45-b5c5-cc60a845ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace values\n",
    "for col_name in replace_cols:\n",
    "    dfwithNo = sparkDF.withColumn(col_name, when(col(col_name)== \"No internet service\",\"No\").otherwise(col(col_name)))\n",
    "    sparkDF = dfwithNo\n",
    "\n",
    "predic = rf_model.transform(dfwithNo)\n",
    "\n",
    "\n",
    "spark.conf.set(\"parentProject\", project_name)\n",
    "bucket = bucket_name\n",
    "spark.conf.set(\"temporaryGcsBucket\",bucket)\n",
    "predic.write.format('bigquery') \\\n",
    ".mode(\"overwrite\")\\\n",
    ".option('table', project_name+':'+dataset_name+'.'+user_name+'_test_output') \\\n",
    ".save()\n",
    "\n",
    "print(predic.show(truncate=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
