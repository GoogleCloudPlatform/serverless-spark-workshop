'''
  Copyright 2023 Google LLC

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 ''' 

#!/usr/bin/env python
# coding: utf-8

from pyspark.sql import SparkSession
import pyspark
from pyspark.ml.classification import MultilayerPerceptronClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.functions import *
from pyspark.ml.feature import VectorAssembler, StringIndexer,OneHotEncoder
from pyspark.ml.classification import LogisticRegression,                    RandomForestClassifier, GBTClassifier
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.sql.types import FloatType
import pyspark.sql.functions as F
from pyspark.ml import Pipeline
from pyspark.ml import PipelineModel
import os
import sys


#Building a Spark session to read and write to and from BigQuery
spark = SparkSession.builder.appName('pyspark-malware-detection-modeling').config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar').getOrCreate()


#Reading the arguments and storing them in variables
project_name=sys.argv[1]
dataset_name=sys.argv[2]
bucket_name=sys.argv[3]
user_name=sys.argv[4]


#Reading network traffic source data for cleanup and preprocessing
sparkDF = spark.read.format('bigquery').load(project_name+':'+dataset_name+'.'+user_name+'_network_traffic_data')

sparkDF.show(20)
sparkDF.printSchema()

stringIndexer = StringIndexer(inputCol='class', outputCol='label')
sparkDF=stringIndexer.fit(sparkDF).transform(sparkDF)


categoricalColumns=['protocol_type','service','flag']
stages = [] # stages in our Pipeline
for categoricalCol in categoricalColumns:
    # Category Indexing with StringIndexer
    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + "Index",handleInvalid="skip")
    # Use OneHotEncoder to convert categorical variables into binary SparseVectors
    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + "classVec"],handleInvalid="keep")
    # Add stages.  These are not run here, but will run all at once later on.
    stages += [stringIndexer, encoder]



# Transform all features into a vector using VectorAssembler
numericCols = ['count',
'logged_in',
'dst_host_same_srv_rate',
'srv_diff_host_rate',
'dst_host_same_src_port_rate',
'srv_serror_rate',
'same_srv_rate',
'dst_host_srv_count',
'hot',
'dst_host_rerror_rate',
'serror_rate',
'dst_host_srv_rerror_rate',
'dst_host_count',
'srv_count',
'dst_host_serror_rate',
'wrong_fragment',
'dst_host_srv_diff_host_rate',
'dst_host_diff_srv_rate',
'src_bytes',
'num_compromised',
'rerror_rate',
'dst_host_srv_serror_rate',
'srv_rerror_rate',
'dst_bytes',
'is_guest_login',
'duration',
'diff_srv_rate',
'num_root',
'num_shells']#'TotalRmbRCN1',
assemblerInputs = numericCols + [c + "classVec" for c in categoricalColumns]
assembler = VectorAssembler(inputCols=assemblerInputs, outputCol="features",handleInvalid="skip")
stages += [assembler]


#Splitting the input data into training and testing sets
(trainingData, testData) = sparkDF.randomSplit([0.7, 0.3], seed=200)


#Building the Random Forest Classification Model
evaluator = MulticlassClassificationEvaluator(labelCol='label',
                                          metricName='accuracy')

rf=RandomForestClassifier(labelCol="label", featuresCol="features")

stages +=[rf]
pipeline_rf = Pipeline(stages=stages)


#Running the model on the training data
model=pipeline_rf.fit(trainingData)


#Running the model on the testing data
prediction = model.transform(testData)
print("Prediction")
prediction.select('features','label','rawPrediction','probability','prediction').show(10)


preds=prediction.select('features','label','probability','prediction')

#Writing the test output data into BigQuery
spark.conf.set("parentProject", project_name)
bucket = bucket_name
spark.conf.set("temporaryGcsBucket",bucket)
preds.write.format('bigquery') .mode("overwrite").option('table', project_name+':'+dataset_name+'.'+user_name+'_model_test_output').save()

preds.show(20)

print(evaluator.evaluate(prediction))


#Saving the Random Forest model to GCS
model.write().overwrite().save('gs://'+bucket_name+'/malware_detection/model/final_model_bigquery')

print('Job Completed Successfully!')
