# Malware Detection using Serverless Spark through Google Cloud Shell

**Goal** - Data Preparation and Model Training for Detecting Malware in Network Dataset.

Following are the lab modules:

[1. Understanding Data](06a_malware_detection_gcloud_execution.md#1-understanding-data)<br>
[2. Solution Architecture](06a_malware_detection_gcloud_execution.md#2-solution-architecture)<br>
[3. Declaring Variables](06a_malware_detection_gcloud_execution.md#3-declaring-variables)<br>
[4. Data Preparation](06a_malware_detection_gcloud_execution.md#4-data-preparation)<br>
[5. Model Training and Testing](06a_malware_detection_gcloud_execution.md#5-model-training-and-testing)<br>
[6. Model Evaluation](06a_malware_detection_gcloud_execution.md#6-model-evaluation)<br>
[7. Logging](06a_malware_detection_gcloud_execution.md#7-logging)<br>

<br>

## 1. Understanding Data

The dataset used for this project is [networks traffic data](01-datasets/network_traffic_dataset.csv). <br>

The dataset contains the following features:

- duration - length (number of seconds) of the connection
- protocol_type - type of the protocol, e.g. tcp, udp, etc.
- service - network service on the destination, e.g., http, telnet, etc.
- flag - normal or error status of the connection
- src_bytes - number of data bytes from source to destination
- dst_bytes - number of data bytes from destination to source
- land - 1 if connection is from/to the same host/port; 0 otherwise
- wrong_fragment - number of wrong fragments
- urgent - number of urgent packets
- hot - number of 'hot' indicators
- num_failed_logins - number of failed login attempts
- logged_in - 1 if successfully logged in; 0 otherwise
- num_compromised - number of compromised conditions
- root_shell - 1 if root shell is obtained; 0 otherwise
- su_attempted - 1 if su root command attempted; 0 otherwise
- num_root - number of root accesses
- num_file_creations - number of file creation operations
- num_shells - number of shell prompts
- num_access_files - number of operations on access control files
- num_outbound_cm ds - number of outbound commands in an ftp session
- is_hot_login - 1 if the login belongs to the hot list; 0 otherwise
- is_guest_login - 1 if the login is a guest login; 0 otherwise
- count - number of connections to the same host as the current connection in the past two seconds
- srv_count - number of connections to the same service as the current connection in the past two seconds
- serror_rate - % of connections that have SYN errors
- srv_serror_rate - % of connections that have SYN errors
- rerror_rate - % of connections that have REJ errors
- srv_rerror_rate - % of connections that have REJ errors
- same_srv_rate - % of connections to the same service
- diff_srv_rate - % of connections to different services
- srv_diff_host_rate - % of connections to different hosts
- dst_host_count - Number of connections having the same destination host IP address.
- dst_host_srv_count - Number of connections having the same port number.
- dst_host_same_srv_rate - % of connections that were to the same service, among the connections aggregated in dst_host_count
- dst_host_diff_srv_rate - % of connections that were to different services, among the connections aggregated in dst_host_count
- dst_host_same_src_port_rate - % of connections that were to the same source port, among the connections aggregated in dst_host_srv_count
- dst_host_srv_diff_host_rate - % of connections that were to different destination machines, among the connections aggregated in dst_host_srv_count
- dst_host_serror_rate - % of connections that have activated the flag (4) s0, s1, s2 or s3, among the connections aggregated in dst_host_count
- dst_host_srv_serror_rate - % of connections that have activated the flag (4) s0, s1, s2 or s3, among the connections aggregated in dst_host_srv_count
- dst_host_rerror_rate  - % of connections that have activated the flag (4) REJ, among the connections aggregated in dst_host_count
- dst_host_srv_rerror_rate - of connections that have activated the flag (4) REJ, among the connections aggregated in dst_host_srv_count
- class - good or bad connection


**Note:** The following features refer to these same-host connections.

- serror_rate
- rerror_rate
- same_srv_rate
- diff_srv_rate
- srv_count

**Note:** The following features refer to these same-service connections.
- srv_serror_rate
- srv_rerror_rate
- srv_diff_host_rate

<br>

## 2. Solution Architecture

<kbd>
<img src=../images/image0.jpg />
</kbd>

<br>
<br>

**Model Pipeline**

The model pipeline involves the following steps:
 - Data cleanup and preparation
 - Building and training a Machine Learning Model (Random Forest Classifier) before saving it into a GCS bucket
 - Using the model built in above step to evaluate test data

<br>

## 3. Declaring Variables

#### 3.1 Set the PROJECT_ID in Cloud Shell

Open Cloud shell or navigate to [shell.cloud.google.com](https://shell.cloud.google.com)<br>
Run the below
```
gcloud config set project $PROJECT_ID

```

#### 3.2 Verify the PROJECT_ID in Cloud Shell

Next, run the following command in cloud shell to ensure that the current project is set correctly:

```
gcloud config get-value project
```

#### 3.3 Declare the variables

Based on the prereqs and checklist, declare the following variables in cloud shell by replacing with your values:


```
PROJECT_ID=$(gcloud config get-value project)       #current GCP project where we are building our use case
REGION=                                             #GCP region where all our resources will be created
SUBNET=                                             #subnet which has private google access enabled
BUCKET_CODE=                                        #GCP bucket where our code, data and model files will be stored
BUCKET_PHS=                                         #bucket where our application logs created in the history server will be stored
HISTORY_SERVER_NAME=                                #name of the history server which will store our application logs
BQ_DATASET_NAME=                                    #BigQuery dataset where all the tables will be stored
UMSA=serverless-spark                               #name of the user managed service account required for the PySpark job executions
SERVICE_ACCOUNT=$UMSA@$PROJECT_ID.iam.gserviceaccount.com
NAME=                                               #Your unique identifier
```

**Note:** For all the variables except 'NAME', please ensure to use the values provided by the admin team.

### 3.4 Update Cloud Shell SDK version

Run the below on cloud shell-

```
gcloud components update
```

<br>

## 4. Data Preparation

Based on EDA, the data preparation script has been created. Among the 42 columns, relevant features have been selected and stored in BQ for the next step of model training.

### 4.1. Run PySpark Serverless Batch for Data Preparation

Run the below on cloud shell -
```
gcloud dataproc batches submit \
  --project $PROJECT_ID \
  --region $REGION \
  pyspark --batch ${NAME}-batch-${RANDOM} \
  gs://$BUCKET_CODE/malware_detection/00-scripts/malware_detection_data_preparation.py \
  --jars gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar \
  --subnet $SUBNET \
  --service-account $SERVICE_ACCOUNT \
  --history-server-cluster projects/$PROJECT_ID/regions/$REGION/clusters/$HISTORY_SERVER_NAME \
  -- $PROJECT_ID $BQ_DATASET_NAME $BUCKET_CODE $NAME
```


### 4.2. Check the output table in BQ

Navigate to BigQuery Console, and check the **malware_detection_lab** dataset. <br>
Once the data preparation batch is completed, a new table '<your_name_here>_network_traffic_data' will be created as shown below :

To view the data in this table -

* Select the table from BigQuery Explorer by navigating 'project_id' **>** 'dataset' **>** 'table_name'
* Click on the **Preview** button to see the data in the table

<br>

<kbd>
<img src=../images/bq_preview.png />
</kbd>

<br>

**Note:** If the **Preview** button is not visible, run the below queries to view the data. However, these queries will be charged for the full table scan.

```
  SELECT * FROM `<project_name>.<dataset_name>.<your_name_here>_network_traffic_data` LIMIT 1000
```

**Note:** Edit all occurrences of <project_name> and <dataset_name> to match the values of the variables PROJECT_ID, and BQ_DATASET_NAME respectively

<kbd>
<img src=../images/image1.png />
</kbd>

<br>

<br>

<br>

## 5. Model Training and Testing

### 5.1. Run PySpark Serverless Batch for Model Training and Testing

The following script will train the model and save the model in the bucket.

Use the gcloud command below:

```
gcloud dataproc batches submit \
  --project $PROJECT_ID \
  --region $REGION \
  pyspark --batch ${NAME}-batch-${RANDOM} \
  gs://$BUCKET_CODE/malware_detection/00-scripts/malware_detection_model_building.py \
  --jars gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar \
  --subnet $SUBNET \
  --service-account $SERVICE_ACCOUNT \
  --history-server-cluster projects/$PROJECT_ID/regions/$REGION/clusters/$HISTORY_SERVER_NAME \
  -- $PROJECT_ID $BQ_DATASET_NAME $BUCKET_CODE $NAME
```

### 5.2. Query the model_test results BQ table

Navigate to BigQuery Console, and check the **malware_detection_lab** dataset. <br>
Once the modelling  batch is completed, a new table '<your_name_here>_model_test_output' will be created as shown below :

To view the data in this table -

* Select the table from BigQuery Explorer by navigating 'project_id' **>** 'dataset' **>** 'table_name'
* Click on the **Preview** button to see the data in the table

<br>

<kbd>
<img src=../images/bq_preview.png />
</kbd>

<br>

**Note:** If the **Preview** button is not visible, run the below queries to view the data. However, these queries will be charged for the full table scan.

```
  SELECT * FROM `<project_name>.<dataset_name>.<your_name_here>_model_test_output` LIMIT 1000;
```
**Note:** Edit all occurrences of <project_name> and <dataset_name> to match the values of the variables PROJECT_ID, and BQ_DATASET_NAME respectively

<kbd>
<img src=../images/image3.png />
</kbd>

<br>
<br>
<br>

## 6. Model Evaluation

### 6.1. Run PySpark Serverless Batch for Model Evaluation

The following script will load the model and predict the new data.

Use the gcloud command below:

```
gcloud dataproc batches submit \
  --project $PROJECT_ID \
  --region $REGION \
  pyspark --batch ${NAME}-batch-${RANDOM} \
  gs://$BUCKET_CODE/malware_detection/00-scripts/malware_detection_model_evaluation.py \
  --jars gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar  \
  --subnet $SUBNET \
  --service-account $SERVICE_ACCOUNT \
  --history-server-cluster projects/$PROJECT_ID/regions/$REGION/clusters/$HISTORY_SERVER_NAME \
  -- $PROJECT_ID $BQ_DATASET_NAME $BUCKET_CODE $NAME
```

### 6.2. Query the model_test results BQ table

Navigate to BigQuery Console, and check the **malware_detection_lab** dataset. <br>
Once the model_testing  batch is completed, a new table '<your_name_here>_model_eval_output' will be created as shown below :

To view the data in this table -

* Select the table from BigQuery Explorer by navigating 'project_id' **>** 'dataset' **>** 'table_name'
* Click on the **Preview** button to see the data in the table

<br>

<kbd>
<img src=../images/bq_preview.png />
</kbd>

<br>

**Note:** If the **Preview** button is not visible, run the below queries to view the data. However, these queries will be charged for the full table scan.

```
  SELECT * FROM `<project_name>.<dataset_name>.<your_name_here>_model_eval_output` LIMIT 1000;
```
**Note:** Edit all occurrences of <project_name> and <dataset_name> to match the values of the variables PROJECT_ID, and BQ_DATASET_NAME respectively

<kbd>
<img src=../images/image7.png />
</kbd>


<br>
<br>
<br>


## 7. Logging

### 7.1 Serverless Batch logs

Logs associated with the application can be found in the logging console under
**Dataproc > Serverless > Batches > <batch_name>**.
<br> You can also click on “View Logs” button on the Dataproc batches monitoring page to get to the logging page for the specific Spark job.

<kbd>
<img src=../images/image10.png />
</kbd>

<kbd>
<img src=../images/image11.png />
</kbd>

<br>

### 7.2 Persistent History Server logs

To view the Persistent History server logs, click the 'View History Server' button on the Dataproc batches monitoring page and the logs will be shown as below:

<br>

<kbd>
<img src=../images/image12.png />
</kbd>

<kbd>
<img src=../images/image13.png />
</kbd>

<br>
