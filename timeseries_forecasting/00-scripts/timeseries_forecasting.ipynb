{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace4e757-78b0-47e4-8421-fb3d961c8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright 2022 Google LLC.\n",
    "#SPDX-License-Identifier: Apache-2.0\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, sum, max, col, concat, lit\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139fa22f-692e-43db-8d8c-65cea6e92257",
   "metadata": {},
   "outputs": [

   ],
   "source": [
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138f34b4-fcbd-4ecb-9548-645e1974bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the arguments and storing them in variables\n",
    "project_name=<<your_project_name>>\n",
    "dataset_name=<<your_dataset_name>>\n",
    "bucket_name=<<your_bucket_name>>\n",
    "user_name=<<your_user_name>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00797b93-c927-4cc5-974d-922976eb33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a spark session\n",
    "spark =SparkSession.builder.appName(\"Timeseries\").config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57aa0793-aee0-43e3-ba78-ce346d4ef068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the output to BigQuery\n",
    "spark.conf.set(\"parentProject\", project_name)\n",
    "bucket = bucket_name\n",
    "spark.conf.set(\"temporaryGcsBucket\",bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af90c9ca-d13d-4e8b-bb29-88bfd49b5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an output schema\n",
    "schema = StructType([\n",
    "        StructField(\"store\", StringType(), True),\n",
    "        StructField(\"item\", StringType(), True),\n",
    "        StructField(\"ds\", DateType(), True),\n",
    "        StructField(\"yhat\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "070ad2a4-26f3-40b8-8b29-53ddb9eb651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fit_pandas_udf(df):\n",
    "    \"\"\"\n",
    "    :param df: Dataframe (train + test data)\n",
    "    :return: predictions as defined in the output schema\n",
    "    \"\"\"\n",
    "\n",
    "    def train_fitted_prophet(df, cutoff):\n",
    "        # train\n",
    "        ts_train = (df\n",
    "                    .query('date <= @cutoff')\n",
    "                    .rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "                    .sort_values('ds')\n",
    "                    )\n",
    "        # test\n",
    "        ts_test = (df\n",
    "                   .query('date > @cutoff')\n",
    "                   .rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "                   .sort_values('ds')\n",
    "                   .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                   .drop('y', axis=1)\n",
    "                   )\n",
    "\n",
    "        print(ts_test.columns)\n",

    "        # init model\n",
    "        m = Prophet(yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=True)\n",
    "        m.fit(ts_train)\n",
    "\n",
    "        # to date\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "        # at this step we predict the future and we get plenty of additional columns be cautious\n",
    "        ts_hat = (m.predict(ts_test)[[\"ds\", \"yhat\"]]\n",
    "                  .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                  ).merge(ts_test, on=[\"ds\"], how=\"left\")  # merge to retrieve item and store index\n",
    "        return pd.DataFrame(ts_hat, columns=schema.fieldNames())\n",
    "\n",
    "    return train_fitted_prophet(df, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64bc54de-8d4e-400b-9755-d87aaa8e12c2",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "tr_df = spark.read.csv('gs://'+bucket_name+'/timeseries_forecasting/01-datasets/train.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06aae9c-44c4-47f6-ae77-88dd09375a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = spark.read.csv('gs://'+bucket_name+'/timeseries_forecasting/01-datasets/test.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ad7346-d0cf-49fa-a2b7-b46717f851d5",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "tr_df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "963f0d20-f7bc-41e4-a22f-2484f9d5ee9e",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "ts_df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c4acd71-9945-4422-9efd-ebedc30bcdd2",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    spark = (SparkSession\n",
    "             .builder\n",
    "             .appName(\"forecasting\")\n",
    "             .config('spark.sql.execution.arrow.enable', 'true')\n",
    "             .getOrCreate()\n",
    "             )\n",
    "\n",
    "    # read input data from :https://www.kaggle.com/c/demand-forecasting-kernels-only/data\n",
    "    data_train = (spark\n",
    "                  .read\n",
    "                  .format(\"csv\")\n",
    "                  .option('header', 'true')\n",
    "                  .load('gs://'+bucket_name+'/timeseries_forecasting/01-datasets/train.csv')\n",
    "                  )\n",
    "\n",
    "    data_test = (spark\n",
    "                 .read\n",
    "                 .format(\"csv\")\n",
    "                 .option('header', 'true')\n",
    "                 .load('gs://'+bucket_name+'/timeseries_forecasting/01-datasets/test.csv')\n",
    "                 .drop('id')\n",
    "                 )\n",
    "    # max train date\n",
    "    cutoff = data_train.select(max(col('date'))).collect()[0][0]\n",
    "    # add sales none col to match with union\n",
    "    data_test = data_test.withColumn('sales', lit(None))\n",
    "    # concat train test\n",
    "    df = (data_train.union(data_test)).sort(col('date'))\n",
    "    # fit\n",
    "    global_predictions = (df\n",
    "                          .groupBy(\"store\", \"item\")\n",
    "                          .apply(fit_pandas_udf)\n",
    "                          )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c20c266c-722b-415b-8947-aee90936adeb",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "global_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "789fe587",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "# write global predictions table to parquet files\n",
    "global_predictions.write.format('bigquery') .mode(\"overwrite\").option('table', project_name+':'+dataset_name+'.'+user_name+'_global_predictions').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4a6c4-f4a9-4aed-a3a3-37f69cf370f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "serverless_spark": "{\"name\":\"projects/tgs-internal-gcpgtminit-dev-01/locations/us-central1/sessions/timeseries_test\",\"uuid\":\"f54aecdb-6641-4f53-94d1-db60e459744e\",\"createTime\":\"2022-06-27T13:15:55.136250Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{\"outputUri\":\"https://mcns23wfmbdy3b4g6ekvedlare-dot-us-central1.dataproc.googleusercontent.com/gateway/default/jupyter/lab/\"},\"state\":\"ACTIVE\",\"stateTime\":\"2022-06-27T13:16:45.952490Z\",\"creator\":\"198454710197-compute@developer.gserviceaccount.com\",\"runtimeConfig\":{\"containerImage\":\"gcr.io/tgs-internal-gcpgtminit-dev-01/timeseries:1.0.1\",\"properties\":{\"spark:spark.jars\":\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar\",\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\"}},\"environmentConfig\":{\"executionConfig\":{\"subnetworkUri\":\"ss-private-subnet-1\"},\"peripheralsConfig\":{\"sparkHistoryServerConfig\":{}}}}",
  "serverless_spark_kernel_name": "remote-2e5d9dad9785fd17e583edb0-pyspark"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
