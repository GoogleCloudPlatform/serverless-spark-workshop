# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 

#!/usr/bin/env python
# coding: utf-8

from pyspark.sql import SparkSession
import pyspark
from pyspark.ml.classification import MultilayerPerceptronClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.functions import *
from pyspark.ml.feature import VectorAssembler, StringIndexer,OneHotEncoder
from pyspark.ml.classification import LogisticRegression,                    RandomForestClassifier, GBTClassifier
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.sql.types import FloatType
import pyspark.sql.functions as F
from pyspark.ml import Pipeline
from pyspark.ml import PipelineModel
import os
import sys


#Building a Spark session to read and write to and from BigQuery
spark = SparkSession.builder.appName('pyspark-malware-detection-modeling').config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar').getOrCreate()


#Reading the arguments and storing them in variables
project_name=sys.argv[1]
dataset_name=sys.argv[2]
bucket_name=sys.argv[3]
user_name=sys.argv[4]


#Reading network traffic source data for cleanup and preprocessing
sparkDF = spark.read.format('bigquery').load(project_name+':'+dataset_name+'.'+user_name+'_network_traffic_data')

sparkDF.show(20)
sparkDF.printSchema()

stringIndexer = StringIndexer(inputCol='class', outputCol='label')
sparkDF=stringIndexer.fit(sparkDF).transform(sparkDF)


categoricalColumns=['protocol_type','service','flag']
stages = [] # stages in our Pipeline
for categoricalCol in categoricalColumns:
    # Category Indexing with StringIndexer
    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + "Index",handleInvalid="skip")
    # Use OneHotEncoder to convert categorical variables into binary SparseVectors
    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + "classVec"],handleInvalid="keep")
    # Add stages.  These are not run here, but will run all at once later on.
    stages += [stringIndexer, encoder]



# Transform all features into a vector using VectorAssembler
numericCols = ['count',
'logged_in',
'dst_host_same_srv_rate',
'srv_diff_host_rate',
'dst_host_same_src_port_rate',
'srv_serror_rate',
'same_srv_rate',
'dst_host_srv_count',
'hot',
'dst_host_rerror_rate',
'serror_rate',
'dst_host_srv_rerror_rate',
'dst_host_count',
'srv_count',
'dst_host_serror_rate',
'wrong_fragment',
'dst_host_srv_diff_host_rate',
'dst_host_diff_srv_rate',
'src_bytes',
'num_compromised',
'rerror_rate',
'dst_host_srv_serror_rate',
'srv_rerror_rate',
'dst_bytes',
'is_guest_login',
'duration',
'diff_srv_rate',
'num_root',
'num_shells']#'TotalRmbRCN1',
assemblerInputs = numericCols + [c + "classVec" for c in categoricalColumns]
assembler = VectorAssembler(inputCols=assemblerInputs, outputCol="features",handleInvalid="skip")
stages += [assembler]


#Splitting the input data into training and testing sets
(trainingData, testData) = sparkDF.randomSplit([0.7, 0.3], seed=200)


#Building the Random Forest Classification Model
evaluator = MulticlassClassificationEvaluator(labelCol='label',
                                          metricName='accuracy')

rf=RandomForestClassifier(labelCol="label", featuresCol="features")

stages +=[rf]
pipeline_rf = Pipeline(stages=stages)


#Running the model on the training data
model=pipeline_rf.fit(trainingData)


#Running the model on the testing data
prediction = model.transform(testData)
print("Prediction")
prediction.select('features','label','rawPrediction','probability','prediction').show(10)


preds=prediction.select('features','label','probability','prediction')

#Writing the test output data into BigQuery
spark.conf.set("parentProject", project_name)
bucket = bucket_name
spark.conf.set("temporaryGcsBucket",bucket)
preds.write.format('bigquery') .mode("overwrite").option('table', project_name+':'+dataset_name+'.'+user_name+'_model_test_output').save()

preds.show(20)

print(evaluator.evaluate(prediction))


#Saving the Random Forest model to GCS
model.write().overwrite().save('gs://'+bucket_name+'/malware_detection/model/final_model_bigquery')

print('Job Completed Successfully!')
