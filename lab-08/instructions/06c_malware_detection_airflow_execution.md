# Malware Detection using Serverless Spark through Airflow.

**Goal** - Data Preparation and Model Training for Detecting Malware in Network Dataset.

Following are the lob modules:

[1. Understanding Data](instructions/06c_malware_detection_airflow_execution.md#1-understanding-data)<br>
[2. Solution Diagram](instructions/06c_malware_detection_airflow_execution.md#2-solution-diagram)<br>
[3. Uploading DAG files to DAGs folder](instructions/06c_malware_detection_airflow_execution.md#3-uploading-dag-files-to-dags-folder)<br>
[4. Execution of Airflow DAG](instructions/06c_malware_detection_airflow_execution.md#4-execution-of-airflow-dag)<br>
[5. BQ output tables](instructions/06c_malware_detection_airflow_execution.md#5-bq-output-tables)<br>
[6. Logging](instructions/06c_malware_detection_airflow_execution.md#6-logging)<br>

<br>

## 1. Understanding Data

The dataset used for this project is [networks traffic data](01-datasets/network_traffic_dataset.csv). <br>

The dataset contains the following features:

- duration - length (number of seconds) of the connection
- protocol_type - type of the protocol, e.g. tcp, udp, etc.
- service - network service on the destination, e.g., http, telnet, etc.
- flag - normal or error status of the connection
- src_bytes - number of data bytes from source to destination
- dst_bytes - number of data bytes from destination to source
- land - 1 if connection is from/to the same host/port; 0 otherwise
- wrong_fragment - number of wrong fragments
- urgent - number of urgent packets
- hot - number of 'hot' indicators
- num_failed_logins - number of failed login attempts
- logged_in - 1 if successfully logged in; 0 otherwise
- num_compromised - number of compromised conditions
- root_shell - 1 if root shell is obtained; 0 otherwise
- su_attempted - 1 if su root command attempted; 0 otherwise
- num_root - number of root accesses
- num_file_creations - number of file creation operations
- num_shells - number of shell prompts
- num_access_files - number of operations on access control files
- num_outbound_cm ds - number of outbound commands in an ftp session
- is_hot_login - 1 if the login belongs to the hot list; 0 otherwise
- is_guest_login - 1 if the login is a guest login; 0 otherwise
- count - number of connections to the same host as the current connection in the past two seconds
- srv_count - number of connections to the same service as the current connection in the past two seconds
- serror_rate - % of connections that have SYN errors
- srv_serror_rate - % of connections that have SYN errors
- rerror_rate - % of connections that have REJ errors
- srv_rerror_rate - % of connections that have REJ errors
- same_srv_rate - % of connections to the same service
- diff_srv_rate - % of connections to different services
- srv_diff_host_rate - % of connections to different hosts
- dst_host_count - Number of connections having the same destination host IP address.
- dst_host_srv_count - Number of connections having the same port number.
- dst_host_same_srv_rate - % of connections that were to the same service, among the connections aggregated in dst_host_count
- dst_host_diff_srv_rate - % of connections that were to different services, among the connections aggregated in dst_host_count
- dst_host_same_src_port_rate - % of connections that were to the same source port, among the connections aggregated in dst_host_srv_count
- dst_host_srv_diff_host_rate - % of connections that were to different destination machines, among the connections aggregated in dst_host_srv_count
- dst_host_serror_rate - % of connections that have activated the flag (4) s0, s1, s2 or s3, among the connections aggregated in dst_host_count
- dst_host_srv_serror_rate - % of connections that have activated the flag (4) s0, s1, s2 or s3, among the connections aggregated in dst_host_srv_count
- dst_host_rerror_rate  - % of connections that have activated the flag (4) REJ, among the connections aggregated in dst_host_count
- dst_host_srv_rerror_rate - of connections that have activated the flag (4) REJ, among the connections aggregated in dst_host_srv_count
- class - good or bad connection


**Note:** The following features refer to these same-host connections.

- serror_rate
- rerror_rate
- same_srv_rate
- diff_srv_rate
- srv_count

**Note:** The following features refer to these same-service connections.
- srv_serror_rate
- srv_rerror_rate
- srv_diff_host_rate

<br>

## 2. Solution Diagram

<kbd>
<img src=../images/Flow_of_Resources.jpeg />
</kbd>

<br>
<br>

**Model Pipeline**

The model pipeline involves the following steps: <br>
	- Create buckets in GCS <br>
	- Create Dataproc and Persistent History Server Cluster <br>
	- Copy the raw data files, PySpark and notebook files into GCS <br>
	- Create a Cloud Composer environment and Airflow jobs to run the serverless spark job <br>
	- Creating Google BigQuery tables  <br>

<br>

## 3. Uploading DAG files to DAGs folder

* From the code repository, download the file located at: **malware_detection**>**00-scripts**>**malware_detection_airflow.py**
* Rename file to <your_name_here>-malware_detection_airflow.py
* Open the file and replace your name on row 21
* Navigate to **Composer**>**<composer_environment>**
* Next, navigate to **Environment Configuration**>**DAGs folder URI**
* Next, upload the DAG file to the GCS bucket corresponding to the **DAGs folder URI**

<kbd>
<img src=../images/composer_2.png />
</kbd>

<br>
<br>
<br>

<kbd>
<img src=../images/composer_3.png />
</kbd>

<br>
<br>
<br>


## 4. Execution of Airflow DAG

* Navigate to **Composer**>**<your_environment>**>**Open Airflow UI**

<kbd>
<img src=../images/composer_5.png />
</kbd>

<br>

* Once the Airflow UI opens, navigate to **DAGs** and open your respective DAG
* Next, trigger your DAG by clicking on the **Trigger DAG** button

<kbd>
<img src=../images/composer_6.png />
</kbd>

<br>

* Once the DAG is triggered, the DAG can be monitored directly through the Airflow UI as well as the Dataproc>Serverless>Batches window

<kbd>
<img src=../images/composer_7.PNG />
</kbd>

<br>

## 5. BQ output tables

Navigate to BigQuery Console, and check the **malware_detection** dataset. <br>
Once the Airflow DAG execution is completed, three new tables '<your_name_here>_network_traffic_data', '<your_name_here>_model_test_output' and '<your_name_here>_model_eval_output' are created.

To view the data in this table -

* Select the table from BigQuery Explorer by navigating 'project_id' **>** 'dataset' **>** 'table_name'
* Click on the **Preview** button to see the data in the table

<br>

<kbd>
<img src=../images/bq_preview.png />
</kbd>

<br>

**Note:** If the **Preview** button is not visible, run the below queries to view the data. However, these queries will be charged for the full table scan.


### 1. Data Preparation Output

```
SELECT * FROM `<project_name>.<dataset_name>.<your_name_here>_network_traffic_data` LIMIT 1000

```

### 2. Model Training and Testing Output

```
  SELECT * FROM `<project_name>.<dataset_name>.<your_name_here>_model_test_output` LIMIT 1000;
```

### 3. Model Evaluation Output

```
  SELECT * FROM `<project_name>.<dataset_name>.<your_name_here>_model_eval_output` LIMIT 1000;
```

**Note:** Edit all occurrences of <project_name> and <dataset_name> to match the values of the variables PROJECT_ID, and BQ_DATASET_NAME respectively

<br>

<kbd>
<img src=../images/image3.png />
</kbd>

<br>

## 6. Logging

### 6.1 Airflow logging

* To view the logs of any step of the DAG execution, click on the **<DAG step>**>**Log** button <br>

<kbd>
<img src=../images/composer_8.png />
</kbd>

<br>

### 6.2 Serverless Batch logs

Logs associated with the application can be found in the logging console under
**Dataproc > Serverless > Batches > <batch_name>**.
<br> You can also click on “View Logs” button on the Dataproc batches monitoring page to get to the logging page for the specific Spark job.

<kbd>
<img src=../images/image10.png />
</kbd>

<kbd>
<img src=../images/image11.png />
</kbd>

<br>

### 6.3 Persistent History Server logs

To view the Persistent History server logs, click the 'View History Server' button on the Dataproc batches monitoring page and the logs will be shown as below:

<br>

<kbd>
<img src=../images/image12.png />
</kbd>

<kbd>
<img src=../images/image13.png />
</kbd>

<br>
