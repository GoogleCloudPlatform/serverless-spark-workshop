{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c91f5-7dd2-49c6-b641-0c8ad5ba73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright 2022 Google LLC.\n",
    "#SPDX-License-Identifier: Apache-2.0\n",
    "#Importing libraries\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, date_format\n",
    "from google.cloud import storage\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd43743-83ff-49ce-9445-6c8e4cbda55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the arguments and storing them in variables\n",
    "project_name=<<your_project_name>>\n",
    "dataset_name=<<your_dataset_name>>\n",
    "bucket_name=<<your_bucket_name>>\n",
    "user_name=<<your_user_name>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69297b3-9915-4979-8f31-988d99b0630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a spark session\n",
    "spark =SparkSession.builder.appName(\"ETLCoviddata\").config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f07ff7-7252-4076-995e-1a5aadbb8d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the output to BigQuery\n",
    "spark.conf.set(\"parentProject\", project_name)\n",
    "bucket = bucket_name\n",
    "spark.conf.set(\"temporaryGcsBucket\",bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b4694-6a30-4d06-88e3-2af09352fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stock data file\n",
    "stock_df = spark.read.options(inferSchema = True, header= True, delimiter=\";\").csv('gs://'+bucket_name+'/covid-economic-impact-vertex-ai/01-datasets/stock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171a456-6ac9-479f-815d-5c62597906e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stringency data file\n",
    "stringency_df = spark.read.options(inferSchema = True, header= True, delimiter=\";\").csv('gs://'+bucket_name+'/covid-economic-impact-vertex-ai/01-datasets/stringency.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa9601-a35a-4d00-98e2-98ff6a52b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create country table\n",
    "country_table = stringency_df.selectExpr('Code as country_code','Country as country').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3590f1-2af5-47db-a93a-499c1b5b4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write country table to parquet files\n",
    "country_table.write.format('bigquery') .mode(\"overwrite\").option('table', project_name+':'+dataset_name+'.'+user_name+'_countries') .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5e4f9-9643-43e6-bb43-0a05b2dbfbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create stock table\n",
    "stock_table = stock_df.selectExpr('Ticker as stock_id','names as company_name','Sector as sector').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42661a-6138-48ad-a310-67c3329865e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write stocks table to parquet files\n",
    "stock_table.write.format('bigquery') .mode(\"overwrite\").option('table', project_name+':'+dataset_name+'.'+user_name+'_stocks') .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cdedc8-c880-4e84-b1f5-0ca49ab50c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time_table\n",
    "time_table = stringency_df.select(['Date']).withColumn('day', dayofmonth('Date')).withColumn('month', month('Date')).withColumn('year', year('Date')).withColumn('weekday', date_format('Date', 'E')).dropDuplicates()\n",
    "time_table.write.format('bigquery') .mode(\"overwrite\").option('table', project_name+':'+dataset_name+'.'+user_name+'_times') .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6f1fe-6969-4385-9abc-4a57095828d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.createOrReplaceTempView(\"stocks\")\n",
    "stringency_df.createOrReplaceTempView(\"stringency\")\n",
    "Ec_status_table = spark.sql(\n",
    "\n",
    "'''SELECT DISTINCT monotonically_increasing_id() as ec_status_id, stringency.Date as date, stringency.Code as country_code, stringency.Stringency_Index as stringency_index, stocks.Ticker as stock_id, stocks.Value_Type as value_type, stocks.Value as value\n",
    "FROM stocks\n",
    "JOIN stringency \n",
    "ON stocks.Date = stringency.Date AND stocks.Country = stringency.Country'''\n",
    "\n",
    ")\n",
    "Ec_status_table.write.format('bigquery') .mode(\"overwrite\").option('table', project_name+':'+dataset_name+'.'+user_name+'_ec_status') .save()\n",
    "\n",
    "print('Job Completed Successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f412d7f-72c1-4ba0-9cbd-ef3d33b3d70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e1858-f4e0-4a31-979f-41f52f75e1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
